{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 1,\n",
      " 'member': 13747,\n",
      " 'meta': 1,\n",
      " 'nd': 281901,\n",
      " 'node': 228786,\n",
      " 'note': 1,\n",
      " 'osm': 1,\n",
      " 'relation': 157,\n",
      " 'tag': 153689,\n",
      " 'way': 37136}\n",
      "{'lower': 92690, 'lower_colon': 57758, 'other': 3241, 'problemchars': 0}\n"
     ]
    }
   ],
   "source": [
    "#Create & Count\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create Sample File\n",
    "OSM_FILE = \"map_AJ.osm\"  # Replace this with your osm file\n",
    "SAMPLE_FILE = \"sample.osm\" \n",
    "\n",
    "\n",
    "# Iterative Parsing\n",
    "\n",
    "\n",
    "# Count Tags\n",
    "def count_tags(filename):\n",
    "    tags = {}\n",
    "    for _, elem in ET.iterparse(filename):\n",
    "        tag = elem.tag\n",
    "        if tag not in tags.keys():\n",
    "            tags[tag] = 1\n",
    "        else:\n",
    "            tags[tag] += 1\n",
    "    return tags\n",
    "\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        for tag in element.iter('tag'):\n",
    "            k = tag.get('k')\n",
    "            if lower.search(element.attrib['k']):\n",
    "                keys['lower'] = keys['lower'] + 1\n",
    "            elif lower_colon.search(element.attrib['k']):\n",
    "                keys['lower_colon'] = keys['lower_colon'] + 1\n",
    "            elif problemchars.search(element.attrib['k']):\n",
    "                keys['problemchars'] = keys['problemchars'] + 1\n",
    "            else:\n",
    "                keys['other'] = keys['other'] + 1\n",
    "    \n",
    "    return keys\n",
    "\n",
    "def key_count(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys\n",
    "\n",
    "\n",
    "# Test count tags function\n",
    "def test():\n",
    "    tags = count_tags(OSM_FILE)\n",
    "    keys = key_count(OSM_FILE)\n",
    "    pprint.pprint(tags)\n",
    "    pprint.pprint(keys)\t\n",
    "\n",
    "    \n",
    "test()\n",
    "\n",
    "OSM_FILE=SAMPLE_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {'Date': {'North Date'},\n",
       "             'Freeway': {'West Agua Fria Freeway', 'West Maricopa Freeway'},\n",
       "             'Ki': {'West Gu u Ki'},\n",
       "             'Ranchos': {'East Camino de Los Ranchos'},\n",
       "             'Sol': {'West Camino del Sol'},\n",
       "             'South': {'East Paradise Village Parkway South'},\n",
       "             'Stewart': {'North Stewart'},\n",
       "             'Sunnyvale': {'South Sunnyvale'},\n",
       "             'Terrace': {'East Sandra Terrace'},\n",
       "             'Verde': {'South Verde'},\n",
       "             'Way': {'East Janice Way',\n",
       "              'East Joseph Way',\n",
       "              'North 102nd Way',\n",
       "              'North 14th Way',\n",
       "              'North 17th Way',\n",
       "              'North 19th Way',\n",
       "              'North 21st Way',\n",
       "              'North 34th Way',\n",
       "              'North 36th Way',\n",
       "              'North 37th Way',\n",
       "              'North 39th Way',\n",
       "              'North 43rd Way',\n",
       "              'North 55th Way',\n",
       "              'North 60th Way',\n",
       "              'North 7th Way',\n",
       "              'North Escobar Way',\n",
       "              'South Larkspur Way',\n",
       "              'West Anthem Way',\n",
       "              'West Carol Ann Way'}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Audit\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Audit file\n",
    "\n",
    "# Audit street names------------------------------------------------------------------------------\n",
    "# Regular expression to check for characters at end of string, including optional period.\n",
    "# Eg \"Street\" or \"St.\"\n",
    "\n",
    "street_type_re = re.compile(r'\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "# Common street names\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Road\", \"Parkway\", \"Commons\", \"Close\", \"Highway\", \"Circle\", \"Trail\", \"US\"]\n",
    "\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.tag == \"tag\") and (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "# Iterate over the osmfile and create a dictionary mapping from expected street names\n",
    "# to collected streets.\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "\n",
    "    osm_file.close()\n",
    "    return street_types   \n",
    "\n",
    "\n",
    "# Check Postalcodes for addresses \n",
    "\n",
    "# Regular expression to check whether postalcode is in appropriate format\n",
    "postcode_re = re.compile('^[A-Z]{1,2}[0-9]{1,2}[A-Z]? [0-9][A-Z]{2}$') \n",
    "\n",
    "def is_postcode(elem):\n",
    "    return (elem.tag == \"tag\") and (elem.attrib['k'] == \"addr:postcode\")\n",
    "\n",
    "\n",
    "# Search for postcodes within \"way\" and \"node\"\n",
    "def find_postcode():\n",
    "    osm_file = open(OSM_FILE, \"r\")\n",
    "    postcode_types = set()\n",
    "    odd_postcode = set()\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_postcode(tag):\n",
    "                    m = postcode_re.search(tag.attrib['v'])\n",
    "                    if m:\n",
    "                        postcode_types.add(tag.attrib['v'])  \n",
    "                    else:\n",
    "                        odd_postcode.add(tag.attrib['v'])\n",
    "                        \n",
    "\n",
    "    osm_file.close()\n",
    "\n",
    "\n",
    "    return (postcode_types, odd_postcode)\n",
    "\n",
    "\n",
    "\n",
    "# Audits\n",
    "audit(OSM_FILE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(set(),\n",
       " {'85003',\n",
       "  '85004',\n",
       "  '85006',\n",
       "  '85007',\n",
       "  '85007-4145',\n",
       "  '85012',\n",
       "  '85013',\n",
       "  '85013-4408',\n",
       "  '85015',\n",
       "  '85015-3809',\n",
       "  '85017',\n",
       "  '85018',\n",
       "  '85019',\n",
       "  '85020',\n",
       "  '85021',\n",
       "  '85022',\n",
       "  '85023',\n",
       "  '85023-1508',\n",
       "  '85023-2301',\n",
       "  '85023-2510',\n",
       "  '85023-8204',\n",
       "  '85024',\n",
       "  '85027',\n",
       "  '85028',\n",
       "  '85029',\n",
       "  '85031',\n",
       "  '85032',\n",
       "  '85034',\n",
       "  '85043',\n",
       "  '85044',\n",
       "  '85045',\n",
       "  '85048',\n",
       "  '85050',\n",
       "  '85051',\n",
       "  '85053',\n",
       "  '85054',\n",
       "  '850822',\n",
       "  '85119',\n",
       "  '85120',\n",
       "  '85142',\n",
       "  '85147',\n",
       "  '85201',\n",
       "  '85203',\n",
       "  '85205',\n",
       "  '85206',\n",
       "  '85207',\n",
       "  '85209',\n",
       "  '85210',\n",
       "  '85212',\n",
       "  '85213',\n",
       "  '85224',\n",
       "  '85248',\n",
       "  '85251',\n",
       "  '85253',\n",
       "  '85254',\n",
       "  '85255',\n",
       "  '85256',\n",
       "  '85260',\n",
       "  '85263',\n",
       "  '85268',\n",
       "  '85281',\n",
       "  '85282',\n",
       "  '85283',\n",
       "  '85286',\n",
       "  '85295',\n",
       "  '85296',\n",
       "  '85297',\n",
       "  '85298',\n",
       "  '85301',\n",
       "  '85302',\n",
       "  '85303',\n",
       "  '85304',\n",
       "  '85305',\n",
       "  '85308',\n",
       "  '85322',\n",
       "  '85323',\n",
       "  '85339',\n",
       "  '85345',\n",
       "  '85351',\n",
       "  '85354',\n",
       "  '85361',\n",
       "  '85374',\n",
       "  '85375',\n",
       "  '85378',\n",
       "  '85381',\n",
       "  '85382',\n",
       "  '85392',\n",
       "  '85396'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_postcode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "North 55th Way => North 55th Way\n",
      "North 55th Way => North 55th Way\n",
      "North 14th Way => North 14th Way\n",
      "North 14th Way => North 14th Way\n",
      "East Joseph Way => East Joseph Way\n",
      "East Joseph Way => East Joseph Way\n",
      "South Larkspur Way => South Larkspur Way\n",
      "South Larkspur Way => South Larkspur Way\n",
      "North 17th Way => North 17th Way\n",
      "North 17th Way => North 17th Way\n",
      "North 43rd Way => North 43rd Way\n",
      "North 43rd Way => North 43rd Way\n",
      "North 37th Way => North 37th Way\n",
      "North 37th Way => North 37th Way\n",
      "North 21st Way => North 21st Way\n",
      "North 21st Way => North 21st Way\n",
      "East Janice Way => East Janice Way\n",
      "East Janice Way => East Janice Way\n",
      "North 39th Way => North 39th Way\n",
      "North 39th Way => North 39th Way\n",
      "North 34th Way => North 34th Way\n",
      "North 34th Way => North 34th Way\n",
      "North 19th Way => North 19th Way\n",
      "North 19th Way => North 19th Way\n",
      "West Carol Ann Way => West Carol Ann Way\n",
      "West Carol Ann Way => West Carol Ann Way\n",
      "North 7th Way => North 7th Way\n",
      "North 7th Way => North 7th Way\n",
      "North 60th Way => North 60th Way\n",
      "North 60th Way => North 60th Way\n",
      "North 36th Way => North 36th Way\n",
      "North 36th Way => North 36th Way\n",
      "West Anthem Way => West Anthem Way\n",
      "West Anthem Way => West Anthem Way\n",
      "North Escobar Way => North Escobar Way\n",
      "North Escobar Way => North Escobar Way\n",
      "North 102nd Way => North 102nd Way\n",
      "North 102nd Way => North 102nd Way\n",
      "West Gu u Ki => West Gu u Ki\n",
      "West Gu u Ki => West Gu u Ki\n",
      "West Camino del Sol => West Camino del Sol\n",
      "West Camino del Sol => West Camino del Sol\n",
      "West Agua Fria Freeway => West Agua Fria Freeway\n",
      "West Agua Fria Freeway => West Agua Fria Freeway\n",
      "West Maricopa Freeway => West Maricopa Freeway\n",
      "West Maricopa Freeway => West Maricopa Freeway\n",
      "South Verde => South Verde\n",
      "South Verde => South Verde\n",
      "East Sandra Terrace => East Sandra Terrace\n",
      "East Sandra Terrace => East Sandra Terrace\n",
      "South Sunnyvale => South Sunnyvale\n",
      "South Sunnyvale => South Sunnyvale\n",
      "North Date => North Date\n",
      "North Date => North Date\n",
      "East Camino de Los Ranchos => East Camino de Los Ranchos\n",
      "East Camino de Los Ranchos => East Camino de Los Ranchos\n",
      "East Paradise Village Parkway South => East Paradise Village Parkway South\n",
      "East Paradise Village Parkway South => East Paradise Village Parkway South\n",
      "North Stewart => North Streetewart\n",
      "North Stewart => North Stewart\n",
      "3:  85392 => 85392\n",
      "3:  85248 => 85248\n",
      "3:  85023-8204 => 85023\n",
      "3:  85396 => 85396\n",
      "3:  85301 => 85301\n",
      "3:  85303 => 85303\n",
      "3:  85302 => 85302\n",
      "3:  85305 => 85305\n",
      "3:  85304 => 85304\n",
      "3:  85308 => 85308\n",
      "3:  85019 => 85019\n",
      "3:  85018 => 85018\n",
      "3:  85268 => 85268\n",
      "3:  85013 => 85013\n",
      "3:  85012 => 85012\n",
      "3:  85015 => 85015\n",
      "3:  85263 => 85263\n",
      "3:  85017 => 85017\n",
      "3:  850822 => 85082\n",
      "3:  85023-2510 => 85023\n",
      "3:  85120 => 85120\n",
      "3:  85253 => 85253\n",
      "3:  85044 => 85044\n",
      "3:  85251 => 85251\n",
      "3:  85006 => 85006\n",
      "3:  85256 => 85256\n",
      "3:  85004 => 85004\n",
      "3:  85254 => 85254\n",
      "3:  85034 => 85034\n",
      "3:  85032 => 85032\n",
      "3:  85031 => 85031\n",
      "3:  85323 => 85323\n",
      "3:  85322 => 85322\n",
      "3:  85043 => 85043\n",
      "3:  85003 => 85003\n",
      "3:  85023-1508 => 85023\n",
      "3:  85020 => 85020\n",
      "3:  85021 => 85021\n",
      "3:  85022 => 85022\n",
      "3:  85023 => 85023\n",
      "3:  85024 => 85024\n",
      "3:  85023-2301 => 85023\n",
      "3:  85027 => 85027\n",
      "3:  85028 => 85028\n",
      "3:  85029 => 85029\n",
      "3:  85007 => 85007\n",
      "3:  85255 => 85255\n",
      "3:  85351 => 85351\n",
      "3:  85224 => 85224\n",
      "3:  85054 => 85054\n",
      "3:  85051 => 85051\n",
      "3:  85050 => 85050\n",
      "3:  85053 => 85053\n",
      "3:  85345 => 85345\n",
      "3:  85013-4408 => 85013\n",
      "3:  85119 => 85119\n",
      "3:  85147 => 85147\n",
      "3:  85339 => 85339\n",
      "3:  85015-3809 => 85015\n",
      "3:  85213 => 85213\n",
      "3:  85212 => 85212\n",
      "3:  85210 => 85210\n",
      "3:  85297 => 85297\n",
      "3:  85296 => 85296\n",
      "3:  85295 => 85295\n",
      "3:  85045 => 85045\n",
      "3:  85260 => 85260\n",
      "3:  85354 => 85354\n",
      "3:  85048 => 85048\n",
      "3:  85298 => 85298\n",
      "3:  85209 => 85209\n",
      "3:  85201 => 85201\n",
      "3:  85203 => 85203\n",
      "3:  85205 => 85205\n",
      "3:  85206 => 85206\n",
      "3:  85207 => 85207\n",
      "3:  85281 => 85281\n",
      "3:  85282 => 85282\n",
      "3:  85283 => 85283\n",
      "3:  85286 => 85286\n",
      "3:  85361 => 85361\n",
      "3:  85381 => 85381\n",
      "3:  85382 => 85382\n",
      "3:  85374 => 85374\n",
      "3:  85375 => 85375\n",
      "3:  85007-4145 => 85007\n",
      "3:  85142 => 85142\n",
      "3:  85378 => 85378\n"
     ]
    }
   ],
   "source": [
    "# Update\n",
    "\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import audit\n",
    "\n",
    "\n",
    "# Mapping for names to be updated\n",
    "mapping = { \" St \": \"Street\",\n",
    "           \"St\": \"Street\",\n",
    "            \"St. \": \"Street\",\n",
    "            \"Blvd\": \"Boulevard\",\n",
    "            \"Ct\": \"Court\",\n",
    "            \"Dr, \": \"Drive \",\n",
    "            \" Dr \": \"Drive \",\n",
    "            \"Dr. \": \"Drive \",\n",
    "            \"Rd\": \"Road\",\n",
    "            \"Rd \": \"Road\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"Pl\": \"Place\",\n",
    "            \"Ave\": \"Avenue \",\n",
    "            \"Ave.\": \"Avenue\",\n",
    "            \"ln \" : \"Lane\",            \n",
    "            \"S \": \"South \",\n",
    "            \"S. \": \" South \",\n",
    "            \"N \": \" North \",\n",
    "            \"N. \": \" North \",\n",
    "            \"W \" : \" West \",\n",
    "            \"W. \": \" West \",\n",
    "            \"E \": \"East \",\n",
    "            \"E. \": \"East \",\n",
    "            \"Hwy \":\"Highway \", \n",
    "           \n",
    "            }\n",
    "\n",
    "# Improving Street names\n",
    "def update_name(name, mapping):\n",
    "    for key in mapping.iterkeys():\n",
    "        if re.search(key, name):\n",
    "            name = re.sub(key, mapping[key], name)\n",
    "\n",
    "    return name\n",
    "\n",
    "def improve_street_name():\n",
    "    st_types = audit.audit(OSM_FILE)   \n",
    "\n",
    "    for st_type, ways in st_types.iteritems():\n",
    "        for name in ways:\n",
    "            better_name = update_name(name, mapping)            \n",
    "            print name, \"=>\", better_name         \n",
    "       \n",
    "            #Second Check replace bad street names with corrected ones       \n",
    "            if True:                 \n",
    "               \n",
    "                better_name = better_name.replace(\" Streetewart\", \" Stewart\")                \n",
    "                print name, \"=>\", better_name \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Noted a postalcode with a +4 number, will drop this and maintain only 9-digit postal code\n",
    "\n",
    "area_postcode_re = re.compile('^[A-Z]{1,2}[0-9]{1,2}[A-Z]? ?[0-9]?$')\n",
    "\n",
    "def update_postcode(odd_postcode):\n",
    "    if area_postcode_re.search(odd_postcode):\n",
    "        postcode = \" \"\n",
    "    else:\n",
    "        postcode = odd_postcode.split(\"-\")[0]        \n",
    "    return postcode\n",
    "    \n",
    "\n",
    "\n",
    "def improve_postcode():\n",
    "    postcode_all = find_postcode()\n",
    "\n",
    "    for postcode in postcode_all[1]:\n",
    "        better_postcode = update_postcode(postcode)\n",
    "        #print \"1: \", postcode, \"=>\", better_postcode\n",
    "        \n",
    "        \n",
    "        #Second Check replace bad zip codes with correct ones       \n",
    "        if True:           \n",
    "            better_postcode = better_postcode.replace(\"085028\", \"85028\")\n",
    "            better_postcode = better_postcode.replace(\"25248\", \"85248\")\n",
    "            better_postcode = better_postcode.replace(\"2804\", \"85204\")\n",
    "            better_postcode = better_postcode.replace(\"5015\", \"85015\")\n",
    "            better_postcode = better_postcode.replace(\"8502\", \"85020\")\n",
    "            better_postcode = better_postcode.replace(\"5015\", \"85015\")     \n",
    "            better_postcode = better_postcode.replace(\"82381\", \"85381\")\n",
    "            better_postcode = better_postcode.replace(\"82158\", \"85015\")\n",
    "            better_postcode = better_postcode.replace(\"84009\", \"85009\")\n",
    "            better_postcode = better_postcode.replace(\"8551\", \"85051\")\n",
    "            better_postcode = better_postcode.replace(\"85331;85377\", \"85331\")\n",
    "            better_postcode = better_postcode.replace(\"84017\", \"85017\") \n",
    "            better_postcode = better_postcode.replace(\"850822\", \"85082\") \n",
    "            #print \"2: \", postcode, \"=>\", better_postcode\n",
    "        \n",
    "        #Third Check replace bad zip codes with correct ones       \n",
    "        if True:\n",
    "            better_postcode = better_postcode.replace(\"850203\", \"85023\")\n",
    "            better_postcode = better_postcode.replace(\"850204\", \"85024\")\n",
    "            better_postcode = better_postcode.replace(\"850201\", \"85021\")\n",
    "            better_postcode = better_postcode.replace(\"850200\", \"85020\")\n",
    "            better_postcode = better_postcode.replace(\"850202\", \"85022\")\n",
    "            better_postcode = better_postcode.replace(\"8885015\", \"85015\")\n",
    "            better_postcode = better_postcode.replace(\"885015\", \"85015\")\n",
    "            better_postcode = better_postcode.replace(\"850822\", \"85082\")\n",
    "            better_postcode = better_postcode.replace(\"850207\", \"85027\")\n",
    "            better_postcode = better_postcode.replace(\"850208\", \"85028\")\n",
    "            better_postcode = better_postcode.replace(\"850209\", \"85029\")\n",
    "            print \"3: \", postcode, \"=>\", better_postcode\n",
    "\n",
    "\n",
    "# Fix Street_names\n",
    "improve_street_name()\n",
    "# Fix postal codes\n",
    "improve_postcode()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export\n",
    "# Data to CSV\n",
    "import csv\n",
    "import codecs\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "from unittest import TestCase\n",
    "import cerberus\n",
    "import schema\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the\n",
    "# sql table schema\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "# Shape each element into several data structures\n",
    "# Clean and shape node or way XML element to Python dict\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \n",
    "    node_attribs = {} \n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []\n",
    "\n",
    "    if element.tag == 'node':\n",
    "        for i in NODE_FIELDS:\n",
    "            node_attribs[i] = element.attrib[i]\n",
    "        for tag in element.iter(\"tag\"):  \n",
    "            problem = PROBLEMCHARS.search(tag.attrib['k'])\n",
    "            if not problem:\n",
    "                node_tag = {} \n",
    "                node_tag['id'] = element.attrib['id'] \n",
    "                node_tag['value'] = tag.attrib['v']  \n",
    "\n",
    "                match = LOWER_COLON.search(tag.attrib['k'])\n",
    "                if not match:\n",
    "                    node_tag['type'] = 'regular'\n",
    "                    node_tag['key'] = tag.attrib['k']\n",
    "                else:\n",
    "                    bef_colon = re.findall('^(.+):', tag.attrib['k'])\n",
    "                    aft_colon = re.findall('^[a-z|_]+:(.+)', tag.attrib['k'])\n",
    "                    node_tag['type'] = bef_colon[0]\n",
    "                    node_tag['key'] = aft_colon[0]\n",
    "                    if node_tag['type'] == \"addr\" and node_tag['key'] == \"street\":\n",
    "                        # update street name\n",
    "                        node_tag['value'] = update_name(tag.attrib['v'], mapping) \n",
    "                    elif node_tag['type'] == \"addr\" and node_tag['key'] == \"postcode\":\n",
    "                        # update post code\n",
    "                        node_tag['value'] = update_postcode(tag.attrib['v']) \n",
    "            tags.append(node_tag)\n",
    "        \n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    \n",
    "    elif element.tag == 'way':\n",
    "        for i in WAY_FIELDS:\n",
    "            way_attribs[i] = element.attrib[i]\n",
    "        for tag in element.iter(\"tag\"):\n",
    "            problem = PROBLEMCHARS.search(tag.attrib['k'])\n",
    "            if not problem:\n",
    "                way_tag = {}\n",
    "                way_tag['id'] = element.attrib['id'] \n",
    "                way_tag['value'] = tag.attrib['v']\n",
    "                match = LOWER_COLON.search(tag.attrib['k'])\n",
    "                if not match:\n",
    "                    way_tag['type'] = 'regular'\n",
    "                    way_tag['key'] = tag.attrib['k']\n",
    "                else:\n",
    "                    bef_colon = re.findall('^(.+?):+[a-z]', tag.attrib['k'])\n",
    "                    aft_colon = re.findall('^[a-z|_]+:(.+)', tag.attrib['k'])\n",
    "\n",
    "                    way_tag['type'] = bef_colon[0]\n",
    "                    way_tag['key'] = aft_colon[0]\n",
    "                    if way_tag['type'] == \"addr\" and way_tag['key'] == \"street\":\n",
    "                        way_tag['value'] = update_name(tag.attrib['v'], mapping) \n",
    "                    elif way_tag['type'] == \"addr\" and way_tag['key'] == \"postcode\":\n",
    "                        way_tag['value'] = update_postcode(tag.attrib['v']) \n",
    "            tags.append(way_tag)\n",
    "        position = 0\n",
    "        for tag in element.iter(\"nd\"):  \n",
    "            nd = {}\n",
    "            nd['id'] = element.attrib['id'] \n",
    "            nd['node_id'] = tag.attrib['ref'] \n",
    "            nd['position'] = position  \n",
    "            position += 1\n",
    "            \n",
    "            way_nodes.append(nd)\n",
    "    \n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Helper Functions            \n",
    "\n",
    "# Yield element if it is the right type of tag\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "# Raise ValidationError if element does not match schema\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "# Extend csv.DictWriter to handle Unicode input\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# Main Function                    \n",
    "\n",
    "# Iteratively process each XML element and write to csv(s)\n",
    "def process_map(file_in, validate):\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    process_map(OSM_FILE, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "table nodes already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e80a066eb1f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# create nodes table\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mcur\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"CREATE TABLE nodes (id, lat, lon, user, uid, version, changeset, timestamp);\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'nodes.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mdr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOperationalError\u001b[0m: table nodes already exists"
     ]
    }
   ],
   "source": [
    "# Create DB\n",
    "import csv, sqlite3\n",
    "\n",
    "con = sqlite3.connect(\"PHX_AZ.db\")\n",
    "con.text_factory = str\n",
    "cur = con.cursor()\n",
    "\n",
    "# create nodes table\n",
    "cur.execute(\"CREATE TABLE nodes (id, lat, lon, user, uid, version, changeset, timestamp);\")\n",
    "with open('nodes.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) \n",
    "    to_db = [(i['id'], i['lat'], i['lon'], i['user'], i['uid'], i['version'], i['changeset'], i['timestamp'])              for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO nodes (id, lat, lon, user, uid, version, changeset, timestamp)                 VALUES (?, ?, ?, ?, ?, ?, ?, ?);\", to_db)\n",
    "con.commit()\n",
    "\n",
    "#create nodes_tags table\n",
    "cur.execute(\"CREATE TABLE nodes_tags (id, key, value, type);\")\n",
    "with open('nodes_tags.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) \n",
    "    to_db = [(i['id'], i['key'], i['value'], i['type']) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO nodes_tags (id, key, value, type) VALUES (?, ?, ?, ?);\", to_db)\n",
    "con.commit()\n",
    "\n",
    "#Create ways table\n",
    "cur.execute(\"CREATE TABLE ways (id, user, uid, version, changeset, timestamp);\")\n",
    "with open('ways.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) \n",
    "    to_db = [(i['id'], i['user'], i['uid'], i['version'], i['changeset'], i['timestamp']) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO ways (id, user, uid, version, changeset, timestamp) VALUES (?, ?, ?, ?, ?, ?);\", to_db)\n",
    "con.commit()\n",
    "\n",
    "#Create ways_nodes table\n",
    "cur.execute(\"CREATE TABLE ways_nodes (id, node_id, position);\")\n",
    "with open('ways_nodes.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) \n",
    "    to_db = [(i['id'], i['node_id'], i['position']) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO ways_nodes (id, node_id, position) VALUES (?, ?, ?);\", to_db)\n",
    "con.commit()\n",
    "\n",
    "#Create ways_tags table\n",
    "cur.execute(\"CREATE TABLE ways_tags (id, key, value, type);\")\n",
    "with open('ways_tags.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) \n",
    "    to_db = [(i['id'], i['key'], i['value'], i['type']) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO ways_tags (id, key, value, type) VALUES (?, ?, ?, ?);\", to_db)\n",
    "con.commit()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query DB\n",
    "import csv, sqlite3\n",
    "\n",
    "def number_of_nodes():\n",
    "    result = cur.execute('SELECT COUNT(*) FROM nodes')\n",
    "    return result.fetchone()[0]\n",
    "\n",
    "def number_of_ways():\n",
    "    result = cur.execute('SELECT COUNT(*) FROM ways')\n",
    "    return result.fetchone()[0]\n",
    "\n",
    "def number_of_Unique_users():\n",
    "    result = cur.execute('SELECT COUNT(distinct(uid)) FROM (SELECT uid FROM nodes UNION ALL SELECT uid FROM ways)')\n",
    "    return result.fetchone()[0]\n",
    "\n",
    "def Top_Contributing_user():\n",
    "    for row in cur.execute('SELECT e.user, COUNT(*) as num                             FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) e                             GROUP BY e.user                             ORDER BY num DESC                             LIMIT 1'):\n",
    "        return row\n",
    "\n",
    "def Biggest_religion():\n",
    "    for row in cur.execute('SELECT nodes_tags.value, COUNT(*) as num FROM nodes_tags                             JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE value=\"place_of_worship\") i                             ON nodes_tags.id=i.id                             WHERE nodes_tags.key=\"religion\"                             GROUP BY nodes_tags.value                             ORDER BY num DESC                            LIMIT 1'):\n",
    "         return row\n",
    "\n",
    "def popular_amenity():\n",
    "    for row in cur.execute('SELECT value, COUNT(*) as num                             FROM nodes_tags                             WHERE key=\"amenity\"                             GROUP BY value                             ORDER BY num DESC                             LIMIT 1'):\n",
    "        return row\n",
    "\n",
    "                \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    con = sqlite3.connect(\"PHX_AZ.db\") \n",
    "    cur = con.cursor()\n",
    "    print \"Number of nodes: \" , number_of_nodes()\n",
    "    print \"Number of ways: \" , number_of_ways()\n",
    "    print \"Number of unique users: \" , number_of_Unique_users()\n",
    "    print \"Top Contributing user: \" , Top_Contributing_user()\n",
    "    print \"Biggest religion: \" , Biggest_religion()\n",
    "    print \"popular amenity: \" , popular_amenity()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
