{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the modules that will be needed\n",
    "from collections import defaultdict\n",
    "import csv \n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "import schema\n",
    "\n",
    "OSM_FILE = \"map_PHX_Metro.osm\"  # Replace this with your osm file\n",
    "SAMPLE_FILE = \"sample.osm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Sample File\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "k = 140 # Parameter: take every k-th top level element 140 for 10k file\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    output.write('<osm>\\n  ')\n",
    "\n",
    "    # Write every kth top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % k == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "    output.write('</osm>')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OSM_FILE = SAMPLE_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'member': 570,\n",
      " 'nd': 52121,\n",
      " 'node': 43512,\n",
      " 'osm': 1,\n",
      " 'relation': 53,\n",
      " 'tag': 31296,\n",
      " 'way': 5974}\n",
      "{'lower': 19122, 'lower_colon': 11005, 'other': 1169, 'problemchars': 0}\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# File\n",
    "OSM_FILE = SAMPLE_FILE \n",
    "\n",
    "\n",
    "# Iterative Parsing\n",
    "# Count Tags\n",
    "def count_tags(filename):\n",
    "    tags = {}\n",
    "    for _, elem in ET.iterparse(filename):\n",
    "        tag = elem.tag\n",
    "        if tag not in tags.keys():\n",
    "            tags[tag] = 1\n",
    "        else:\n",
    "            tags[tag] += 1\n",
    "    return tags\n",
    "\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        for tag in element.iter('tag'):\n",
    "            k = tag.get('k')\n",
    "            if lower.search(element.attrib['k']):\n",
    "                keys['lower'] = keys['lower'] + 1\n",
    "            elif lower_colon.search(element.attrib['k']):\n",
    "                keys['lower_colon'] = keys['lower_colon'] + 1\n",
    "            elif problemchars.search(element.attrib['k']):\n",
    "                keys['problemchars'] = keys['problemchars'] + 1\n",
    "            else:\n",
    "                keys['other'] = keys['other'] + 1\n",
    "    \n",
    "    return keys\n",
    "\n",
    "def key_count(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys\n",
    "\n",
    "\n",
    "# Test count tags function\n",
    "def test():\n",
    "    tags = count_tags(OSM_FILE)\n",
    "    keys = key_count(OSM_FILE)\n",
    "    \n",
    "    pprint.pprint(tags)\n",
    "    pprint.pprint(keys)\t\n",
    "\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audit file\n",
    "\n",
    "# Audit street names------------------------------------------------------------------------------\n",
    "# Regular expression to check for characters at end of string, including optional period.\n",
    "# Eg \"Street\" or \"St.\"\n",
    "\n",
    "street_type_re = re.compile(r'\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "# Common street names\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Road\", \"Parkway\", \"Freeway\", \"Close\", \"Highway\", \"Circle\", \"Trail\", \"US\", \"60\"]\n",
    "\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.tag == \"tag\") and (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "# Iterate over the osmfile and create a dictionary mapping from expected street names\n",
    "# to collected streets.\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "\n",
    "    osm_file.close()\n",
    "    return street_types   \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {'Date': {'North Date'},\n",
       "             'Ki': {'West Gu u Ki'},\n",
       "             'Ranchos': {'East Camino de Los Ranchos'},\n",
       "             'Sol': {'West Camino del Sol'},\n",
       "             'South': {'East Paradise Village Parkway South'},\n",
       "             'Stewart': {'North Stewart'},\n",
       "             'Sunnyvale': {'South Sunnyvale'},\n",
       "             'Terrace': {'East Sandra Terrace'},\n",
       "             'Verde': {'South Verde'},\n",
       "             'Way': {'East Janice Way',\n",
       "              'East Joseph Way',\n",
       "              'North 102nd Way',\n",
       "              'North 14th Way',\n",
       "              'North 17th Way',\n",
       "              'North 19th Way',\n",
       "              'North 21st Way',\n",
       "              'North 34th Way',\n",
       "              'North 36th Way',\n",
       "              'North 37th Way',\n",
       "              'North 39th Way',\n",
       "              'North 43rd Way',\n",
       "              'North 55th Way',\n",
       "              'North 60th Way',\n",
       "              'North 7th Way',\n",
       "              'North Escobar Way',\n",
       "              'South Larkspur Way',\n",
       "              'West Anthem Way',\n",
       "              'West Carol Ann Way'}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test audit function\n",
    "audit(OSM_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping for names to be updated\n",
    "mapping = { \" St \": \"Street\",\n",
    "           \"St\": \"Street\",\n",
    "            \"St. \": \"Street\",\n",
    "            \"Blvd\": \"Boulevard\",\n",
    "            \"Ct\": \"Court\",\n",
    "            \"Dr, \": \"Drive \",\n",
    "            \" Dr \": \"Drive \",\n",
    "            \"Dr. \": \"Drive \",\n",
    "            \"Rd\": \"Road\",\n",
    "            \"Rd \": \"Road\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"Pl\": \"Place\",\n",
    "            \"Ave\": \"Avenue \",\n",
    "            \"Ave.\": \"Avenue\",\n",
    "            \"ln \" : \"Lane\",            \n",
    "            \"S \": \"South \",\n",
    "            \"S. \": \" South \",\n",
    "            \"N \": \" North \",\n",
    "            \"N. \": \" North \",\n",
    "            \"W \" : \" West \",\n",
    "            \"W. \": \" West \",\n",
    "            \"E \": \"East \",\n",
    "            \"E. \": \"East \",\n",
    "            \"Hwy \":\"Highway \", \n",
    "            \"S L\": \"South L\",\n",
    "            \"n Bon\": \"North Bon\",           \n",
    "            \", Superior, AZ 85173\": \"West US-60\",\n",
    "            \"5810 A\":\"A\",\n",
    "            \"0 w D\":\"0 West D\"            \n",
    "            }\n",
    "\n",
    "# Improving Street names\n",
    "def update_name(name, mapping):\n",
    "    for key in mapping.iterkeys():\n",
    "        if re.search(key, name):\n",
    "            name = re.sub(key, mapping[key], name)\n",
    "\n",
    "    return name\n",
    "\n",
    "def improve_street_name():\n",
    "    st_types = audit(OSM_FILE)   \n",
    "\n",
    "    for st_type, ways in st_types.iteritems():\n",
    "        for name in ways:\n",
    "            better_name = update_name(name, mapping)            \n",
    "            print name, \"=>\", better_name         \n",
    "       \n",
    "            #Second Check replace bad street names with corrected ones       \n",
    "            if True: \n",
    "                \n",
    "                better_name = better_name.replace(\"Road.\", \"Road\")\n",
    "                better_name = better_name.replace(\"393 \", \"\")\n",
    "                better_name = better_name.replace(\", Chandler, AZ 85225\", \"\")\n",
    "                better_name = better_name.replace(\" nueida\", \"\")\n",
    "                better_name = better_name.replace(\" nue\", \"\")\n",
    "                \n",
    "                better_name = better_name.replace(\" nue#158.\", \"\")\n",
    "                better_name = better_name.replace(\" ue`\", \"\")\n",
    "                better_name = better_name.replace(\"  nueue\", \"\")\n",
    "                better_name = better_name.replace(\"  nueue'\", \"\")\n",
    "                better_name = better_name.replace(\"Streetart\", \"Street\")\n",
    "                \n",
    "                better_name = better_name.replace(\"Streete 4\", \"\")\n",
    "                better_name = better_name.replace(\"#158\", \"\")                \n",
    "                better_name = better_name.replace(\" #100\", \"\")\n",
    "                better_name = better_name.replace(\", Suite A\", \"\")\n",
    "                better_name = better_name.replace(\", Building B\", \"\")\n",
    "                better_name = better_name.replace(\" #B\", \"\")\n",
    "                better_name = better_name.replace(\" C1\", \"\")\n",
    "                better_name = better_name.replace(\", Suite 34\", \"\")\n",
    "                better_name = better_name.replace(\" #900\", \"\")\n",
    "                better_name = better_name.replace(\". #1205\", \"\")\n",
    "                better_name = better_name.replace(\" Suite D\", \"\")\n",
    "               \n",
    "                better_name = better_name.replace(\"West US-60West US-60\", \"West US-60\")\n",
    "                better_name = better_name.replace(\"a Dr\", \"a Drive\") \n",
    "                better_name = better_name.replace(\"e Dr\", \"e Drive\")\n",
    "                better_name = better_name.replace(\"t Dr\", \"t Drive\")\n",
    "                better_name = better_name.replace(\"y Dr\", \"y Drive\")\n",
    "                better_name = better_name.replace(\"x Dr\", \"x Drive\")\n",
    "                better_name = better_name.replace(\" Dr.\", \" Drive\")\n",
    "                better_name = better_name.replace(\"Circle Driveive\", \"Circle Drive\")\n",
    "                better_name = better_name.replace(\"Vista Driveive\", \"Vista Drive\")\n",
    "                better_name = better_name.replace(\"Village Driveive\", \"Village Drive\")\n",
    "                better_name = better_name.replace(\"Norte Dr.\", \"Norte Drive\")\n",
    "                better_name = better_name.replace(\" Streetick\", \" Stick\")\n",
    "                better_name = better_name.replace(\" Pkwy\", \" Parkway\")               \n",
    "                better_name = better_name.replace(\" Stadium\", \" Stadium\")\n",
    "                better_name = better_name.replace(\" lane\", \" Lane\")\n",
    "                print name, \"=>\", better_name \n",
    "                \n",
    "            #Third Check replace bad street names with corrected ones       \n",
    "            if True:\n",
    "                better_name = better_name.replace(\" Avenueue`\", \" Avenue\")\n",
    "                better_name = better_name.replace(\" Avenueue\", \" Avenue\")\n",
    "                better_name = better_name.replace(\" Streetewart\", \" Stewart\")                \n",
    "                print name, \"=>\", better_name \n",
    "\n",
    "  \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "North 55th Way => North 55th Way\n",
      "North 55th Way => North 55th Way\n",
      "North 55th Way => North 55th Way\n",
      "North 14th Way => North 14th Way\n",
      "North 14th Way => North 14th Way\n",
      "North 14th Way => North 14th Way\n",
      "East Joseph Way => East Joseph Way\n",
      "East Joseph Way => East Joseph Way\n",
      "East Joseph Way => East Joseph Way\n",
      "South Larkspur Way => South Larkspur Way\n",
      "South Larkspur Way => South Larkspur Way\n",
      "South Larkspur Way => South Larkspur Way\n",
      "North 17th Way => North 17th Way\n",
      "North 17th Way => North 17th Way\n",
      "North 17th Way => North 17th Way\n",
      "North 43rd Way => North 43rd Way\n",
      "North 43rd Way => North 43rd Way\n",
      "North 43rd Way => North 43rd Way\n",
      "North 37th Way => North 37th Way\n",
      "North 37th Way => North 37th Way\n",
      "North 37th Way => North 37th Way\n",
      "North 21st Way => North 21st Way\n",
      "North 21st Way => North 21st Way\n",
      "North 21st Way => North 21st Way\n",
      "East Janice Way => East Janice Way\n",
      "East Janice Way => East Janice Way\n",
      "East Janice Way => East Janice Way\n",
      "North 39th Way => North 39th Way\n",
      "North 39th Way => North 39th Way\n",
      "North 39th Way => North 39th Way\n",
      "North 34th Way => North 34th Way\n",
      "North 34th Way => North 34th Way\n",
      "North 34th Way => North 34th Way\n",
      "North 19th Way => North 19th Way\n",
      "North 19th Way => North 19th Way\n",
      "North 19th Way => North 19th Way\n",
      "West Carol Ann Way => West Carol Ann Way\n",
      "West Carol Ann Way => West Carol Ann Way\n",
      "West Carol Ann Way => West Carol Ann Way\n",
      "North 7th Way => North 7th Way\n",
      "North 7th Way => North 7th Way\n",
      "North 7th Way => North 7th Way\n",
      "North 60th Way => North 60th Way\n",
      "North 60th Way => North 60th Way\n",
      "North 60th Way => North 60th Way\n",
      "North 36th Way => North 36th Way\n",
      "North 36th Way => North 36th Way\n",
      "North 36th Way => North 36th Way\n",
      "West Anthem Way => West Anthem Way\n",
      "West Anthem Way => West Anthem Way\n",
      "West Anthem Way => West Anthem Way\n",
      "North Escobar Way => North Escobar Way\n",
      "North Escobar Way => North Escobar Way\n",
      "North Escobar Way => North Escobar Way\n",
      "North 102nd Way => North 102nd Way\n",
      "North 102nd Way => North 102nd Way\n",
      "North 102nd Way => North 102nd Way\n",
      "West Gu u Ki => West Gu u Ki\n",
      "West Gu u Ki => West Gu u Ki\n",
      "West Gu u Ki => West Gu u Ki\n",
      "West Camino del Sol => West Camino del Sol\n",
      "West Camino del Sol => West Camino del Sol\n",
      "West Camino del Sol => West Camino del Sol\n",
      "South Verde => South Verde\n",
      "South Verde => South Verde\n",
      "South Verde => South Verde\n",
      "East Sandra Terrace => East Sandra Terrace\n",
      "East Sandra Terrace => East Sandra Terrace\n",
      "East Sandra Terrace => East Sandra Terrace\n",
      "South Sunnyvale => South Sunnyvale\n",
      "South Sunnyvale => South Sunnyvale\n",
      "South Sunnyvale => South Sunnyvale\n",
      "North Date => North Date\n",
      "North Date => North Date\n",
      "North Date => North Date\n",
      "East Camino de Los Ranchos => East Camino de Los Ranchos\n",
      "East Camino de Los Ranchos => East Camino de Los Ranchos\n",
      "East Camino de Los Ranchos => East Camino de Los Ranchos\n",
      "East Paradise Village Parkway South => East Paradise Village Parkway South\n",
      "East Paradise Village Parkway South => East Paradise Village Parkway South\n",
      "East Paradise Village Parkway South => East Paradise Village Parkway South\n",
      "North Stewart => North Streetewart\n",
      "North Stewart => North Streetewart\n",
      "North Stewart => North Stewart\n"
     ]
    }
   ],
   "source": [
    "# Clean streets \n",
    "improve_street_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Postalcodes for addresses \n",
    "\n",
    "# Regular expression to check whether postalcode is in appropriate format\n",
    "postcode_re = re.compile('^[A-Z]{1,2}[0-9]{1,2}[A-Z]? [0-9][A-Z]{2}$') \n",
    "\n",
    "def is_postcode(elem):\n",
    "    return (elem.tag == \"tag\") and (elem.attrib['k'] == \"addr:postcode\")\n",
    "\n",
    "\n",
    "# Search for postcodes within \"way\" and \"node\"\n",
    "def find_postcode():\n",
    "    osm_file = open(OSM_FILE, \"r\")\n",
    "    postcode_types = set()\n",
    "    odd_postcode = set()\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_postcode(tag):\n",
    "                    m = postcode_re.search(tag.attrib['v'])\n",
    "                    if m:\n",
    "                        postcode_types.add(tag.attrib['v'])  \n",
    "                    else:\n",
    "                        odd_postcode.add(tag.attrib['v'])\n",
    "                        \n",
    "\n",
    "    osm_file.close()\n",
    "\n",
    "\n",
    "    return (postcode_types, odd_postcode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(set(),\n",
       " {'85003',\n",
       "  '85004',\n",
       "  '85006',\n",
       "  '85007',\n",
       "  '85007-4145',\n",
       "  '85012',\n",
       "  '85013',\n",
       "  '85013-4408',\n",
       "  '85015',\n",
       "  '85015-3809',\n",
       "  '85017',\n",
       "  '85018',\n",
       "  '85019',\n",
       "  '85020',\n",
       "  '85021',\n",
       "  '85022',\n",
       "  '85023',\n",
       "  '85023-1508',\n",
       "  '85023-2301',\n",
       "  '85023-2510',\n",
       "  '85023-8204',\n",
       "  '85024',\n",
       "  '85027',\n",
       "  '85028',\n",
       "  '85029',\n",
       "  '85031',\n",
       "  '85032',\n",
       "  '85034',\n",
       "  '85043',\n",
       "  '85044',\n",
       "  '85045',\n",
       "  '85048',\n",
       "  '85050',\n",
       "  '85051',\n",
       "  '85053',\n",
       "  '85054',\n",
       "  '850822',\n",
       "  '85119',\n",
       "  '85120',\n",
       "  '85142',\n",
       "  '85147',\n",
       "  '85201',\n",
       "  '85203',\n",
       "  '85205',\n",
       "  '85206',\n",
       "  '85207',\n",
       "  '85209',\n",
       "  '85210',\n",
       "  '85212',\n",
       "  '85213',\n",
       "  '85224',\n",
       "  '85248',\n",
       "  '85251',\n",
       "  '85253',\n",
       "  '85254',\n",
       "  '85255',\n",
       "  '85256',\n",
       "  '85260',\n",
       "  '85263',\n",
       "  '85268',\n",
       "  '85281',\n",
       "  '85282',\n",
       "  '85283',\n",
       "  '85286',\n",
       "  '85295',\n",
       "  '85296',\n",
       "  '85297',\n",
       "  '85298',\n",
       "  '85301',\n",
       "  '85302',\n",
       "  '85303',\n",
       "  '85304',\n",
       "  '85305',\n",
       "  '85308',\n",
       "  '85322',\n",
       "  '85323',\n",
       "  '85339',\n",
       "  '85345',\n",
       "  '85351',\n",
       "  '85354',\n",
       "  '85361',\n",
       "  '85374',\n",
       "  '85375',\n",
       "  '85378',\n",
       "  '85381',\n",
       "  '85382',\n",
       "  '85392',\n",
       "  '85396'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Audit postal codes\n",
    "find_postcode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noted a postalcode with a +4 number, will drop this and maintain only 9-digit postal code\n",
    "\n",
    "area_postcode_re = re.compile('^[A-Z]{1,2}[0-9]{1,2}[A-Z]? ?[0-9]?$')\n",
    "\n",
    "def update_postcode(odd_postcode):\n",
    "    if area_postcode_re.search(odd_postcode):\n",
    "        postcode = \" \"\n",
    "    else:\n",
    "        postcode = odd_postcode.split(\"-\")[0]        \n",
    "    return postcode\n",
    "    \n",
    "\n",
    "\n",
    "def improve_postcode():\n",
    "    postcode_all = find_postcode()\n",
    "\n",
    "    for postcode in postcode_all[1]:\n",
    "        better_postcode = update_postcode(postcode)\n",
    "        #print \"1: \", postcode, \"=>\", better_postcode\n",
    "        \n",
    "        \n",
    "        #Second Check replace bad zip codes with correct ones       \n",
    "        if True:           \n",
    "            better_postcode = better_postcode.replace(\"085028\", \"85028\")\n",
    "            better_postcode = better_postcode.replace(\"25248\", \"85248\")\n",
    "            better_postcode = better_postcode.replace(\"2804\", \"85204\")\n",
    "            better_postcode = better_postcode.replace(\"5015\", \"85015\")\n",
    "            better_postcode = better_postcode.replace(\"8502\", \"85020\")\n",
    "            better_postcode = better_postcode.replace(\"5015\", \"85015\")     \n",
    "            better_postcode = better_postcode.replace(\"82381\", \"85381\")\n",
    "            better_postcode = better_postcode.replace(\"82158\", \"85015\")\n",
    "            better_postcode = better_postcode.replace(\"84009\", \"85009\")\n",
    "            better_postcode = better_postcode.replace(\"8551\", \"85051\")\n",
    "            better_postcode = better_postcode.replace(\"85331;85377\", \"85331\")\n",
    "            better_postcode = better_postcode.replace(\"84017\", \"85017\") \n",
    "            better_postcode = better_postcode.replace(\"850822\", \"85082\") \n",
    "            #print \"2: \", postcode, \"=>\", better_postcode\n",
    "        \n",
    "        #Third Check replace bad zip codes with correct ones       \n",
    "        if True:\n",
    "            better_postcode = better_postcode.replace(\"850203\", \"85023\")\n",
    "            better_postcode = better_postcode.replace(\"850204\", \"85024\")\n",
    "            better_postcode = better_postcode.replace(\"850201\", \"85021\")\n",
    "            better_postcode = better_postcode.replace(\"850200\", \"85020\")\n",
    "            better_postcode = better_postcode.replace(\"850202\", \"85022\")\n",
    "            better_postcode = better_postcode.replace(\"8885015\", \"85015\")\n",
    "            better_postcode = better_postcode.replace(\"885015\", \"85015\")\n",
    "            better_postcode = better_postcode.replace(\"850822\", \"85082\")\n",
    "            better_postcode = better_postcode.replace(\"850207\", \"85027\")\n",
    "            better_postcode = better_postcode.replace(\"850208\", \"85028\")\n",
    "            better_postcode = better_postcode.replace(\"850209\", \"85029\")\n",
    "            print \"3: \", postcode, \"=>\", better_postcode\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3:  85392 => 85392\n",
      "3:  85248 => 85248\n",
      "3:  85023-8204 => 85023\n",
      "3:  85396 => 85396\n",
      "3:  85301 => 85301\n",
      "3:  85303 => 85303\n",
      "3:  85302 => 85302\n",
      "3:  85305 => 85305\n",
      "3:  85304 => 85304\n",
      "3:  85308 => 85308\n",
      "3:  85019 => 85019\n",
      "3:  85018 => 85018\n",
      "3:  85268 => 85268\n",
      "3:  85013 => 85013\n",
      "3:  85012 => 85012\n",
      "3:  85015 => 85015\n",
      "3:  85263 => 85263\n",
      "3:  85017 => 85017\n",
      "3:  850822 => 85082\n",
      "3:  85023-2510 => 85023\n",
      "3:  85120 => 85120\n",
      "3:  85253 => 85253\n",
      "3:  85044 => 85044\n",
      "3:  85251 => 85251\n",
      "3:  85006 => 85006\n",
      "3:  85256 => 85256\n",
      "3:  85004 => 85004\n",
      "3:  85254 => 85254\n",
      "3:  85034 => 85034\n",
      "3:  85032 => 85032\n",
      "3:  85031 => 85031\n",
      "3:  85323 => 85323\n",
      "3:  85322 => 85322\n",
      "3:  85043 => 85043\n",
      "3:  85003 => 85003\n",
      "3:  85023-1508 => 85023\n",
      "3:  85020 => 85020\n",
      "3:  85021 => 85021\n",
      "3:  85022 => 85022\n",
      "3:  85023 => 85023\n",
      "3:  85024 => 85024\n",
      "3:  85023-2301 => 85023\n",
      "3:  85027 => 85027\n",
      "3:  85028 => 85028\n",
      "3:  85029 => 85029\n",
      "3:  85007 => 85007\n",
      "3:  85255 => 85255\n",
      "3:  85351 => 85351\n",
      "3:  85224 => 85224\n",
      "3:  85054 => 85054\n",
      "3:  85051 => 85051\n",
      "3:  85050 => 85050\n",
      "3:  85053 => 85053\n",
      "3:  85345 => 85345\n",
      "3:  85013-4408 => 85013\n",
      "3:  85119 => 85119\n",
      "3:  85147 => 85147\n",
      "3:  85339 => 85339\n",
      "3:  85015-3809 => 85015\n",
      "3:  85213 => 85213\n",
      "3:  85212 => 85212\n",
      "3:  85210 => 85210\n",
      "3:  85297 => 85297\n",
      "3:  85296 => 85296\n",
      "3:  85295 => 85295\n",
      "3:  85045 => 85045\n",
      "3:  85260 => 85260\n",
      "3:  85354 => 85354\n",
      "3:  85048 => 85048\n",
      "3:  85298 => 85298\n",
      "3:  85209 => 85209\n",
      "3:  85201 => 85201\n",
      "3:  85203 => 85203\n",
      "3:  85205 => 85205\n",
      "3:  85206 => 85206\n",
      "3:  85207 => 85207\n",
      "3:  85281 => 85281\n",
      "3:  85282 => 85282\n",
      "3:  85283 => 85283\n",
      "3:  85286 => 85286\n",
      "3:  85361 => 85361\n",
      "3:  85381 => 85381\n",
      "3:  85382 => 85382\n",
      "3:  85374 => 85374\n",
      "3:  85375 => 85375\n",
      "3:  85007-4145 => 85007\n",
      "3:  85142 => 85142\n",
      "3:  85378 => 85378\n"
     ]
    }
   ],
   "source": [
    "# Fix postal codes\n",
    "improve_postcode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "def get_user(element):\n",
    "    return\n",
    "\n",
    "# Generates list of users\n",
    "def process_users(filename):\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        name = element.attrib.get('user')\n",
    "        if name != None:\n",
    "            if name not in users:\n",
    "                users.add(name)\n",
    "                \n",
    "    pass\n",
    "    return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pluton_od',\n",
       " 'scharith',\n",
       " 'AndyAyre',\n",
       " 'Nick Coury',\n",
       " 'nairnidh',\n",
       " 'CMOC',\n",
       " 'nedluxaca',\n",
       " 'ygudeti',\n",
       " 'Sundance',\n",
       " 'Brian@Brea',\n",
       " 'ashleyannmathew',\n",
       " 'West-MEC',\n",
       " 'hereisam',\n",
       " 'srsjojo',\n",
       " 'chagasru',\n",
       " 'ashradha',\n",
       " 'Pixelcrunch',\n",
       " 'frizatch',\n",
       " 'fun60CxMR',\n",
       " 'vasamd',\n",
       " 'RichRico',\n",
       " 'deejayh',\n",
       " 'wallywall',\n",
       " 'antimeridian',\n",
       " 'soundred',\n",
       " 'ammhita',\n",
       " 'hemasvn',\n",
       " 'ramyaragupathy',\n",
       " 'syrah1',\n",
       " 'LilyVilla',\n",
       " 'Johnny Five',\n",
       " 'Joe2oh',\n",
       " 'bhavant',\n",
       " 'sskalyan',\n",
       " 'clay_c',\n",
       " 'GeneBase',\n",
       " 'LML013',\n",
       " 'Mar Mar',\n",
       " 'aurel_joys',\n",
       " 'bradrh',\n",
       " 'boiker',\n",
       " 'bswitzman',\n",
       " 'Kent Shaffer',\n",
       " 'yurasi',\n",
       " 'NicWow',\n",
       " 'Pnrrth',\n",
       " 'TomHoman',\n",
       " 'tusnayak',\n",
       " 'maxerickson',\n",
       " 'tejajt',\n",
       " 'tiwdeves',\n",
       " 'jaywhyenecks',\n",
       " 'RichAZICN',\n",
       " 'Korgi1',\n",
       " 'thetornado76',\n",
       " 'sun0123',\n",
       " 'harinikh',\n",
       " 'Cuken',\n",
       " 'MapperMudkip',\n",
       " 'tomthepom',\n",
       " 'racranji',\n",
       " 'sawhs',\n",
       " 'Adam Schneider',\n",
       " 'pkkv',\n",
       " 'harshjs',\n",
       " 'MeltMyMind09',\n",
       " 'IAmScottCarlson',\n",
       " 'istvanv_telenav',\n",
       " 'sunilvs',\n",
       " 'Geo_Lugo',\n",
       " 'pezizomycotina',\n",
       " 'knalette',\n",
       " 'nfgusedautoparts',\n",
       " 'Easky30',\n",
       " 'AMMAZ',\n",
       " 'ArizonaMapper',\n",
       " 'kvss',\n",
       " 'jcmak182',\n",
       " 'ShatteredArm',\n",
       " 'harishwr',\n",
       " 'EarnYourBacon',\n",
       " 'Zakened',\n",
       " 'HJUdall',\n",
       " 'ctr112280',\n",
       " 'DiamondDevil',\n",
       " u'Volker Fr\\xf6hlich',\n",
       " 'Kate Varf',\n",
       " 'jonathanpatt',\n",
       " 'riteshaw',\n",
       " 'amznruth',\n",
       " 'Raihas',\n",
       " 'gsubhan',\n",
       " 'beweta',\n",
       " 'Minh Nguyen',\n",
       " 'HoloDuke',\n",
       " 'LeTopographeFou',\n",
       " 'vbbukka',\n",
       " 'flannel365',\n",
       " 'Central Control',\n",
       " 'jedidy',\n",
       " 'pcroque',\n",
       " 'floki',\n",
       " 'bbovi',\n",
       " 'jogger333',\n",
       " 'Paul Hahn',\n",
       " 'Zeric',\n",
       " 'Shadowclaimer',\n",
       " u'\\u042e\\u043a\\u0430\\u0442\\u0430\\u043d',\n",
       " 'arosalon',\n",
       " 'b1n9',\n",
       " 'JacobPayne214',\n",
       " 'Hjart',\n",
       " 'oldtopos',\n",
       " 'Mikala Chaffee',\n",
       " 'herbm123',\n",
       " 'woodpeck_repair',\n",
       " 'Spanholz',\n",
       " 'kghazi',\n",
       " 'rekasruj',\n",
       " 'metaniq',\n",
       " 'McPhat',\n",
       " 'babj615',\n",
       " 'hasnabak',\n",
       " u'\\u0417\\u0435\\u043b\\u0451\\u043d\\u044b\\u0439 \\u041a\\u043e\\u0448\\u0430\\u043a',\n",
       " 'jomsjaco',\n",
       " 'almaasm',\n",
       " 'amarajz',\n",
       " 'knikitha',\n",
       " 'Lchater',\n",
       " 'anunlikelycloud',\n",
       " 'joc005',\n",
       " 'gsteinmon',\n",
       " 'ediyes',\n",
       " 'namannik',\n",
       " 'nmixter',\n",
       " 'ramsaivr',\n",
       " 'linkemup',\n",
       " 'OpenStreetMapper5',\n",
       " 'sctrojan79',\n",
       " 'ravsjith',\n",
       " 'vatsomya',\n",
       " 'Sarr_Cat',\n",
       " 'Hvieira83',\n",
       " 'sriharsd',\n",
       " 'Cradleguard',\n",
       " 'jima72',\n",
       " 'mansipan',\n",
       " 'n76',\n",
       " 'uboot',\n",
       " 'Sunny Nair',\n",
       " 'arkdatta',\n",
       " 'Ryan Shotwell',\n",
       " 'sean2019alt',\n",
       " 'kuavula',\n",
       " 'ksrilekh',\n",
       " 'grandhs',\n",
       " 'Harrison Allen-Sutter',\n",
       " 'apburnes',\n",
       " 'MrRagerPager',\n",
       " 'blazeman',\n",
       " 'lrw2_lyft',\n",
       " 'utkshukl',\n",
       " 'us-az-mesa-UGBL',\n",
       " '1CarlosAguiar',\n",
       " 'Chaos99',\n",
       " 'aryabhatta',\n",
       " 'ErichRitz',\n",
       " 'MMendoza3569',\n",
       " 'chandeek',\n",
       " 'mappeurbzh',\n",
       " 'aridzona1',\n",
       " 'vysyarp',\n",
       " 'Karthoo',\n",
       " 'matthieun',\n",
       " 'rebot',\n",
       " 'jojoyal',\n",
       " 'bhiy',\n",
       " 'ejjoshy',\n",
       " 'rojganes',\n",
       " '217541OSM',\n",
       " 'MikeN',\n",
       " 'fragler',\n",
       " 'thelonedolan',\n",
       " 'Sushil2828',\n",
       " 'borvikas',\n",
       " 'befit1',\n",
       " 'Thadekam',\n",
       " 'newantt',\n",
       " 'RGimbel',\n",
       " 'mcdonn123',\n",
       " 'sabaamzn',\n",
       " 'rathjoth',\n",
       " 'redgnana',\n",
       " 'dwatling',\n",
       " 'amandastanko',\n",
       " 'boddushr',\n",
       " 'gireeshn',\n",
       " 'srirohan',\n",
       " 'TeshMusili',\n",
       " 'Notinarea',\n",
       " 'BillyTLowry',\n",
       " 'Ropino',\n",
       " 'muziriana',\n",
       " 'isabelle h',\n",
       " 'Duff614',\n",
       " 'michael colvin',\n",
       " 'ruthmaben',\n",
       " '01archerdave',\n",
       " 'jacobbraeutigam',\n",
       " 'Nikhilnetr',\n",
       " 'Your Village Maps',\n",
       " 'kanleela',\n",
       " 'tne',\n",
       " 'Zartbitter',\n",
       " 'abosworth10',\n",
       " 'ender2851',\n",
       " 'datbrahm',\n",
       " 'mcingara',\n",
       " 'sekta1',\n",
       " 'amithrav',\n",
       " 'Rajsamand Local Guide',\n",
       " 'somehow_different',\n",
       " 'vnixon',\n",
       " 'kollipay',\n",
       " '4b696d',\n",
       " 'animebirder',\n",
       " 'iggujja',\n",
       " 'srmudava',\n",
       " 'vanguruh',\n",
       " 'srygy',\n",
       " 'Adamant1',\n",
       " 'bathis',\n",
       " 'Sampool15',\n",
       " 'Viki_travel_weak',\n",
       " 'sushredd',\n",
       " 'F_H_Howler',\n",
       " 'SaiNithin',\n",
       " 'clarecorthell',\n",
       " 'tahuram',\n",
       " 'lukas64',\n",
       " 'kennethdale1969',\n",
       " 'vextina',\n",
       " 'JeffB-AZ',\n",
       " 'mapwjs',\n",
       " 'jalajm',\n",
       " 'DesertNavigator',\n",
       " 'White_Rabbit',\n",
       " 'jangahari',\n",
       " 'pandypr',\n",
       " 'Dilys',\n",
       " 'ynglt',\n",
       " 'Caleb2002',\n",
       " 'bcskillings',\n",
       " 'shkshar',\n",
       " 'eagsc7',\n",
       " 'yadathot',\n",
       " 'arehaan',\n",
       " 'Glassman',\n",
       " 'niet3sche77',\n",
       " 'wdbspephd',\n",
       " 'greggerm',\n",
       " 'andygol',\n",
       " 'RosieHuck',\n",
       " 'kbzimmer',\n",
       " 'WhiteRabbitEXE',\n",
       " 'sreejam',\n",
       " 'rayn',\n",
       " 'Pjspaws',\n",
       " 'jharpster',\n",
       " 'akalohri',\n",
       " 'AZFF',\n",
       " 'srividya_c',\n",
       " 'sgudiva',\n",
       " 'dchiles',\n",
       " 'Jon Hanson',\n",
       " 'kkre1',\n",
       " 'skodeboi',\n",
       " 'korky99_04',\n",
       " 'manuelab_telenav',\n",
       " 'Lambertus',\n",
       " 'KanaLee',\n",
       " 'Megan A',\n",
       " 'robgeb',\n",
       " 'Little Brother',\n",
       " 'vangalb',\n",
       " 'dviraja',\n",
       " 'vkanduku',\n",
       " 'supergeekdave',\n",
       " 'davidtheclark',\n",
       " u'Fant\\xf4mas',\n",
       " 'Mike Coulson',\n",
       " 'kaurrupa',\n",
       " 'goraia',\n",
       " 'nnpriyn',\n",
       " 'manings',\n",
       " 'mueschel',\n",
       " 'javmoh',\n",
       " 'wierz11',\n",
       " 'LeaTaklaja',\n",
       " 'Azsnow90',\n",
       " 'Rupert Swarbrick',\n",
       " 'leelamrs',\n",
       " 'danhooker',\n",
       " 'kasbadri',\n",
       " 'Marvin Herbold',\n",
       " 'Nathan921',\n",
       " 'Clark Andrews',\n",
       " 'adamos',\n",
       " 'rahuzod',\n",
       " 'duarisha',\n",
       " 'skquinn',\n",
       " 'pyram',\n",
       " 'asrdd',\n",
       " 'Suidakra',\n",
       " 'user_7700954',\n",
       " 'biancah_telenav',\n",
       " 'xujiayi_1256',\n",
       " 'amillar',\n",
       " 'Luiyo',\n",
       " 'fjtrock',\n",
       " 'ccompanik',\n",
       " 'daduncan',\n",
       " 'chekasat',\n",
       " 'Stalfur',\n",
       " 'friendBOMBER',\n",
       " 'hz46033',\n",
       " 'dekatherm',\n",
       " 'jollybubbly',\n",
       " 'Joschi518',\n",
       " '25or6to4',\n",
       " 'BigDaveAz',\n",
       " 'naresksv',\n",
       " 'redtitan',\n",
       " 'b-jazz-bot',\n",
       " 'darkonus',\n",
       " 'chrysrobyn',\n",
       " 'California Bear',\n",
       " 'DubC',\n",
       " 'devapujk',\n",
       " 'kkoganti',\n",
       " 'gollabg',\n",
       " 'TheAwesomeHwyh',\n",
       " 'bibasra',\n",
       " 'Lazerhawk16',\n",
       " 'erikalynn',\n",
       " 'dkokosky',\n",
       " 'Peter Davies',\n",
       " 'mzamam',\n",
       " 'antoalvi',\n",
       " 'SimMoonXP',\n",
       " 'Vkkalegi',\n",
       " 'nmotupal',\n",
       " 'loyakris',\n",
       " 'swathiku',\n",
       " 'ToffeHoff',\n",
       " 'RLS123',\n",
       " 'GoNinja',\n",
       " 'shadty',\n",
       " 'pathkh',\n",
       " 'TheSolarGuy',\n",
       " 'h4rpur',\n",
       " 'orphion28',\n",
       " 'sggundl',\n",
       " 'RoadGeek_MD99',\n",
       " 'Bike Mapper',\n",
       " 'quintonmoran',\n",
       " 'praghath',\n",
       " 'tux16090',\n",
       " 'marinad_telenav',\n",
       " 'jonesydesign',\n",
       " 'jacorohi',\n",
       " 'samguer',\n",
       " 'Richard Holzer',\n",
       " 'JulienBalas',\n",
       " 'Magick93',\n",
       " 'Globe4Change',\n",
       " 'Jacobenz',\n",
       " 'fun60Cx',\n",
       " 'Rav4kumar',\n",
       " 'choppe224',\n",
       " 'sandhill',\n",
       " 'jyotkuma',\n",
       " 'mdk',\n",
       " 'Grant Anderson',\n",
       " 'bongaram',\n",
       " 'GunStarOne',\n",
       " 'StellanL',\n",
       " 'Shaun@Asu',\n",
       " 'bmmh',\n",
       " 'FF808',\n",
       " 'brycedb74',\n",
       " 'sonmanek',\n",
       " u'Miss\\u6c14\\u5b81\\u8f69\\u6602',\n",
       " 'triggwill',\n",
       " 'Guylamar2006',\n",
       " 'MisterPhilip',\n",
       " 'Sartpro',\n",
       " 'baigzake',\n",
       " 'sanuhya',\n",
       " 'changurl',\n",
       " 'penimire',\n",
       " 'srinatpa',\n",
       " 'Airlobster',\n",
       " 'manitec',\n",
       " 'Alan Trick',\n",
       " 'Zazzmatazz',\n",
       " '2bre99',\n",
       " 'gauraan',\n",
       " 'aarp65',\n",
       " 'kumartcx',\n",
       " 'alphasquid',\n",
       " 'abvincen',\n",
       " 'mrfa',\n",
       " 'mvexel',\n",
       " 'jamenimm',\n",
       " 'agnisn',\n",
       " 'sunilaak',\n",
       " 'DustoneGT',\n",
       " 'nstefan_telenav',\n",
       " 'amitbish',\n",
       " 'ahabdu',\n",
       " 'artscars',\n",
       " 'majumdea',\n",
       " 'Jasonrun',\n",
       " 'bapup',\n",
       " 'JosueBar',\n",
       " 'asanad',\n",
       " 'Tonyaz5005',\n",
       " 'IotosCa',\n",
       " 'Alexey Vazhnov',\n",
       " 'GREGMAP1',\n",
       " 'US Woods',\n",
       " 'aaronsta',\n",
       " 'adenium',\n",
       " 'TBogle',\n",
       " 'inah_telenav',\n",
       " 'TTUOSM14',\n",
       " 'pattamaa',\n",
       " 'badxhampill',\n",
       " 'vissushm',\n",
       " 'hoserab',\n",
       " 'Byung Kyu Park',\n",
       " 'sartsuri',\n",
       " 'mapalla',\n",
       " 'Maxzilla1954',\n",
       " 'Alyona Naboka',\n",
       " 'Shimazina',\n",
       " 'derricknehrenberg',\n",
       " 'russdeffner',\n",
       " 'melisramer',\n",
       " 'ganarend',\n",
       " 'nandinab',\n",
       " 'bryceco',\n",
       " 'Fungurume Man',\n",
       " 'Gomez23',\n",
       " 'Adam Martin',\n",
       " 'RockerDadAZ',\n",
       " 'kwametedros',\n",
       " 'Flugzeug149',\n",
       " 'tammpava',\n",
       " 'CSanders0',\n",
       " 'Clarke22',\n",
       " 'Danrh',\n",
       " 'mahahahaneapneap',\n",
       " 'freebeer',\n",
       " 'lorandr_telenav',\n",
       " 'sgmamz',\n",
       " 'Turtur',\n",
       " 'zmankits',\n",
       " 'GRailMapper',\n",
       " 'bnilima',\n",
       " 'Latze',\n",
       " 'PhysicsArmature',\n",
       " 'Hatem613',\n",
       " 'coffeena',\n",
       " 'Caboosey',\n",
       " 'lilamatilda',\n",
       " 'ELadner',\n",
       " 'vvrddy',\n",
       " 'wmcconahey',\n",
       " 'Tim_the_toolman',\n",
       " 'valherum',\n",
       " 'kookiemonster',\n",
       " 'mbiker',\n",
       " 'pratikyadav',\n",
       " 'MadManX',\n",
       " 'siayu',\n",
       " 'alexsweeney',\n",
       " 'scchevel',\n",
       " 'smbharan',\n",
       " 'mohapd',\n",
       " 'avssr',\n",
       " 'rkachelriess',\n",
       " 'veesams',\n",
       " 'parkerw13',\n",
       " 'cdm23',\n",
       " 'Revanaes',\n",
       " 'Naayan_khare',\n",
       " 'StevenJ',\n",
       " 'tyos',\n",
       " 'ialex',\n",
       " 'nambim',\n",
       " 'augxcgrwxo',\n",
       " 'byronigoe',\n",
       " 'cougsnoopy',\n",
       " 'GreggTownsend',\n",
       " 'Taylor Sapero',\n",
       " 'chdr',\n",
       " 'dvvuyyur',\n",
       " 'fx99',\n",
       " 'indukun',\n",
       " 'azmapper08',\n",
       " 'buchula',\n",
       " 'nomad@ASU',\n",
       " 'atomwaffen',\n",
       " 'AZ_Hiker',\n",
       " 'abaker_lyft',\n",
       " 'crimsongosu',\n",
       " 'renvoid',\n",
       " 'Margori123',\n",
       " 'maps-neu',\n",
       " 'MatthewAndersonUS80',\n",
       " '_jket',\n",
       " 'nbhiss',\n",
       " 'nihaalr',\n",
       " 'gausserrorfunction',\n",
       " 'Kurly',\n",
       " 'swarnimv',\n",
       " 'nehnaaz',\n",
       " 'neuhausr',\n",
       " 'vaidhev',\n",
       " 'Sanguinarius',\n",
       " 'wallclimber21',\n",
       " 'carlosp10',\n",
       " 'doublah',\n",
       " 'Jim Johansen',\n",
       " 'mihaii_telenav',\n",
       " 'Ico656',\n",
       " 'chaturvh',\n",
       " 'gsravya1',\n",
       " 'KindredCoda',\n",
       " 'vennredd',\n",
       " 'sivapaid',\n",
       " 'bhanuban',\n",
       " 'DennisL',\n",
       " 'ChrisDMI',\n",
       " 'DaveHansenTiger',\n",
       " 'harpreo',\n",
       " 'Matlas',\n",
       " 'Richard',\n",
       " 'GISCADfan',\n",
       " 'nishisi',\n",
       " 'stephmogo',\n",
       " 'sivaib',\n",
       " 'thorhale',\n",
       " 'steadfasthello',\n",
       " 'ElliottPlack',\n",
       " 'accheela',\n",
       " 'bjnlm',\n",
       " 'MapperMudkip_import',\n",
       " 'Ethan4Joh',\n",
       " 'Leif Lodahl',\n",
       " 'swarood',\n",
       " 'PhQ',\n",
       " 'Arlo James Barnes',\n",
       " 'pbobbili',\n",
       " 'techlady',\n",
       " 'pillalp',\n",
       " 'flierfy',\n",
       " 'ParagonPrime',\n",
       " 'gappleto97',\n",
       " 'Kongel_0',\n",
       " 'shark_dentist96',\n",
       " 'PlaneMad',\n",
       " 'FrogMouse',\n",
       " 'Target92',\n",
       " 'saikariv',\n",
       " 'ppallam',\n",
       " 'rldutch1',\n",
       " 'gilasri',\n",
       " 'pddondet',\n",
       " 'Rusty Gate',\n",
       " 'gaku',\n",
       " 'lucasm08',\n",
       " 'Alex-7',\n",
       " 'sunaman',\n",
       " 'Sunny Thaper',\n",
       " 'riskhann',\n",
       " 'SpazTheCat26',\n",
       " 'mohahase',\n",
       " 'rmessenger',\n",
       " 'DontMapMeBro',\n",
       " 'saisuri',\n",
       " 'ZoidTP',\n",
       " 'malpans',\n",
       " 'TurnDriverSide',\n",
       " 'mwakram',\n",
       " 'ssange',\n",
       " 'debott',\n",
       " 'Rand0m1',\n",
       " 'gazsri',\n",
       " 'James Fee',\n",
       " 'ridixcr',\n",
       " 'Kevin Ruiz-Conforme',\n",
       " 'Neverwhere',\n",
       " 'tmadhuli',\n",
       " 'Nehaj',\n",
       " 'Valustaides',\n",
       " 'vchndl',\n",
       " 'Omnific',\n",
       " 'buddhirv',\n",
       " 'ruph',\n",
       " 'Parker M Shea',\n",
       " 'MikeChuck',\n",
       " 'Canyonsrcool',\n",
       " 'GerdP',\n",
       " 'Nate Wessel',\n",
       " 'anukrits',\n",
       " 'mr_catman',\n",
       " 'KielbasaHolmes',\n",
       " 'talukdm',\n",
       " 'Extinct',\n",
       " 'Maria Fish',\n",
       " 'naaitha',\n",
       " 'HeyItsAdam',\n",
       " 'swappa',\n",
       " 'Baloo Uriza',\n",
       " 'kevmath',\n",
       " 'matty44m',\n",
       " 'Andre68',\n",
       " 'Diego Sanguinetti',\n",
       " 'boopington',\n",
       " 'Su Ah',\n",
       " 'homeslice60148',\n",
       " 'appank',\n",
       " 'mbasinge',\n",
       " 'b-jazz',\n",
       " 'Meadsashi',\n",
       " 'Timothy Smith',\n",
       " 'AZDave63',\n",
       " 'dedNikifor',\n",
       " 'enthused91',\n",
       " 'Jacob Hackamack',\n",
       " 'karitotp',\n",
       " 'Vlad',\n",
       " 'sreelekk',\n",
       " 'schultzy1087',\n",
       " 'Chris Lawrence',\n",
       " 'utkap',\n",
       " 'TorCguy',\n",
       " 'autocorr',\n",
       " 'mirror176',\n",
       " 'ramijcr',\n",
       " 'Kittenz',\n",
       " 'ashishub',\n",
       " 'alixhartmann',\n",
       " 'Bob Tole',\n",
       " 'PlacesOfInterest2887',\n",
       " 'Tap Estes',\n",
       " 'corb555',\n",
       " 'unnsyeda',\n",
       " 'Bhojaraj',\n",
       " 'rkkarnee',\n",
       " 'shadijan',\n",
       " 'saxvidit',\n",
       " 'Shawn Dowler',\n",
       " 'vpprahar',\n",
       " 'Salena12',\n",
       " 'brrolson',\n",
       " 'srigirip',\n",
       " 'lurlrlrl',\n",
       " 'sevarg11',\n",
       " 'ibanez37',\n",
       " 'Ryan Ewing',\n",
       " 'kirashas',\n",
       " 'hongshuyao',\n",
       " 'aghazi',\n",
       " 'KingSaguaro',\n",
       " 'ammmze',\n",
       " 'limmmoh',\n",
       " 'debanka',\n",
       " 'ctsjohnz',\n",
       " 'pppadal',\n",
       " 'darylb',\n",
       " 'simplyjav',\n",
       " 'poodoshi',\n",
       " 'karragha',\n",
       " 'ortho_is_hot',\n",
       " 'ahecht415',\n",
       " 'Sunfishtommy',\n",
       " 'hrikrib',\n",
       " 'movemus',\n",
       " 'ramayang',\n",
       " 'mycota',\n",
       " 'sankethn',\n",
       " 'Cjacob57',\n",
       " 'Alan Bragg',\n",
       " 'mattspecker',\n",
       " 'alaskansan',\n",
       " 'nimmalab',\n",
       " 'pflier',\n",
       " 'laharicl',\n",
       " 'ereiamjh3',\n",
       " '!i!',\n",
       " 'panchis1',\n",
       " 'FrankieG02',\n",
       " 'katakp',\n",
       " 'Joshua Garner',\n",
       " 'Awesomesauce',\n",
       " 'AndrewSnow',\n",
       " 'rivermont',\n",
       " 'nick_lyft',\n",
       " 'OutdatedVersion',\n",
       " 'rithikaj',\n",
       " 'maponpoint33',\n",
       " 'northofparadise',\n",
       " 'Jen0815',\n",
       " 'Arroju',\n",
       " 'rickmastfan67',\n",
       " 'nathan206',\n",
       " 'swimdb',\n",
       " 'kotaprad',\n",
       " 'MrBill2',\n",
       " 'Dr Kludge{import}',\n",
       " 'brianegge',\n",
       " 'shawat94',\n",
       " 'ermoser603',\n",
       " 'shreeush',\n",
       " 'kirrawat',\n",
       " 'brendann',\n",
       " 'NW757',\n",
       " 'kammann',\n",
       " 'Jonah',\n",
       " 'SomeoneElse_Revert',\n",
       " 'Iqhra',\n",
       " 'desuman',\n",
       " 'Echo Echo',\n",
       " 'vvvuppal',\n",
       " 'AKLAB',\n",
       " 'jkingham',\n",
       " 'MichaelCollinson',\n",
       " 'navuddin',\n",
       " 'riysingh',\n",
       " 'amearai',\n",
       " 'WernerP',\n",
       " 'Fa7C0N',\n",
       " 'sivachak',\n",
       " 'TaedeT',\n",
       " 'Constable',\n",
       " u'IbnT\\u0113\\u0161f\\u012bn',\n",
       " 'Chetan_Gowda',\n",
       " 'jschaub',\n",
       " 'kfremd',\n",
       " 'dasnei',\n",
       " 'gujelapg',\n",
       " 'salvationcookie',\n",
       " 'madreag',\n",
       " 'Computerfreaked',\n",
       " 'HTMLSpinnr',\n",
       " 'Sardellas cactus',\n",
       " 'siddhaam',\n",
       " 'dwh1985',\n",
       " 'Bytemark',\n",
       " 'squigglyspooge',\n",
       " 'himabudd',\n",
       " 'karl-marx',\n",
       " 'LoftyGames',\n",
       " 'Boppet',\n",
       " 'anurasi',\n",
       " 'chepurb',\n",
       " 'Iowa Kid',\n",
       " 'Cool_DPS',\n",
       " 'micha_k',\n",
       " 'Helmchen42',\n",
       " 'rkkasams',\n",
       " 'wambacher',\n",
       " 'ComeSailAway',\n",
       " 'varshia',\n",
       " 'chhava',\n",
       " 'Gkieta',\n",
       " 'Dr Kludge',\n",
       " 'Bkissin',\n",
       " 'themrpotatohead',\n",
       " 'xepshnlone',\n",
       " 'braty',\n",
       " 'TheBlueRobin',\n",
       " 'DurangoJay',\n",
       " 'rawatg',\n",
       " 'SathyaPendyala',\n",
       " 'DannyAiquipa',\n",
       " 'ptaot',\n",
       " 'bingo',\n",
       " 'Andrey Kuznetsov2018',\n",
       " 'wvdp',\n",
       " 'patadivy',\n",
       " 'Brian Reavis',\n",
       " 'Runs-4-cache',\n",
       " 'silakshm',\n",
       " 'balrog-kun',\n",
       " 'losthiker',\n",
       " 'vorpalblade',\n",
       " 'revent',\n",
       " 'hno2',\n",
       " 'rppalagu',\n",
       " 'HokiePilot',\n",
       " 'xe1gyp',\n",
       " 'bauherr',\n",
       " 'Space Godzilla',\n",
       " 'stadiaarcadia',\n",
       " 'David Maciaszek',\n",
       " 'catswift26',\n",
       " 'inagans',\n",
       " 'Luis36995',\n",
       " 'Thyais Meade',\n",
       " 'bhardwk',\n",
       " 'pdantojh',\n",
       " 'bahnpirat',\n",
       " 'paasthan',\n",
       " 'Andrei-Bondarev',\n",
       " 'Reino Baptista',\n",
       " 'Daiji Maps',\n",
       " 'shoshibu',\n",
       " 'oliviap_telenav',\n",
       " 'mar2860mr',\n",
       " 'daganzdaanda',\n",
       " 'calfarome',\n",
       " 'somanch',\n",
       " 'sireeshp',\n",
       " 'rrnju',\n",
       " 'firsky',\n",
       " 'amzmkuma',\n",
       " 'cmdigital',\n",
       " 'modukv',\n",
       " 'vkatredd',\n",
       " 'Christian70',\n",
       " 'pieman011',\n",
       " 'jaiaka',\n",
       " 'Carnildo',\n",
       " 'ommundu',\n",
       " 'Quiott',\n",
       " 'lost707',\n",
       " 'Hummingbird77',\n",
       " 'kllahari',\n",
       " 'xBalthamel',\n",
       " 'Ember McCall',\n",
       " 'jumbanho',\n",
       " 'asddiqu',\n",
       " 'Cato_d_Ae',\n",
       " 'NeverGuy',\n",
       " 'amanura',\n",
       " 'alsaihn',\n",
       " 'venkakaz',\n",
       " 'hofoen',\n",
       " 'CorranHorn',\n",
       " 'shhys',\n",
       " 'andreis_telenav',\n",
       " 'malenki',\n",
       " 'Ahmed Altaf',\n",
       " 'The Temecula Mapper',\n",
       " 'woodpeck_fixbot',\n",
       " 'dreadfyre',\n",
       " 'Rudolf Mayer',\n",
       " 'EdSS',\n",
       " 'michaedz',\n",
       " 'marthaleena',\n",
       " 'troyras673',\n",
       " 'Sammy57',\n",
       " '_jcaruso',\n",
       " 'pbellamk',\n",
       " 'sandchi',\n",
       " 'chachafish',\n",
       " 'rmburkhead',\n",
       " 'rekulc1',\n",
       " '@kevin_bullock',\n",
       " 'OSMuser47',\n",
       " 'Freedom Editor',\n",
       " 'mahmedqg',\n",
       " 'sebastic',\n",
       " 'elk valley',\n",
       " 'aggopak',\n",
       " 'sinreeya',\n",
       " 'Rootberg',\n",
       " 'presakha',\n",
       " 'T_9er',\n",
       " 'samely',\n",
       " '7im',\n",
       " 'crtrue25',\n",
       " 'Steve',\n",
       " 'Eric McIntyre',\n",
       " 'Putt-For-Doe',\n",
       " 'navlay',\n",
       " 'effektz',\n",
       " 'devratc',\n",
       " 'whatanaxhole',\n",
       " 'petrar_telenav',\n",
       " 'Cichlid78',\n",
       " 'eric22',\n",
       " 'EvanDotPro',\n",
       " 'MtarTDi',\n",
       " 'Trex2001',\n",
       " 'Andi39',\n",
       " 'The Reveler',\n",
       " 'SBadger',\n",
       " 'salisburymistake',\n",
       " 'Benutzername187',\n",
       " 'leandrok',\n",
       " 'snewb96',\n",
       " 'DeVietor',\n",
       " 'TorhamZed',\n",
       " 'upendrakarukonda',\n",
       " 'Toyotaracer81',\n",
       " 'Reemal_lyft',\n",
       " 'dyellak',\n",
       " u'Andr\\xe9s Ram\\xedrez',\n",
       " 'colindt',\n",
       " 'tejkante',\n",
       " 'clydew297',\n",
       " 'ayushupreti22',\n",
       " 'sabodige',\n",
       " 'flockfinder',\n",
       " 'RustProof Labs',\n",
       " 'Ehcko',\n",
       " 'Patricia Solis',\n",
       " 'prafupan',\n",
       " 'dabearis',\n",
       " 'ChaosStopper',\n",
       " 'gdodlapa',\n",
       " 'deanabil',\n",
       " 'pete404',\n",
       " 'Brandon Reavis',\n",
       " 'himansne',\n",
       " 'payelgh',\n",
       " 'chepteja',\n",
       " 'Lisa Mccraw',\n",
       " 'KailaKaukaua',\n",
       " u'Waldseem\\xfcller',\n",
       " 'ctine89',\n",
       " 'teliks',\n",
       " 'Bryan_W',\n",
       " 'snyash',\n",
       " 'anilredd',\n",
       " 'mathpras',\n",
       " 'Hsmorg',\n",
       " 'Nick_T',\n",
       " 'MartyMartPa',\n",
       " 'AlexeyTyur',\n",
       " 'hoream_telenav',\n",
       " 'addatla',\n",
       " 'mvviveka',\n",
       " 'catalinad_telenav',\n",
       " 'chosayan',\n",
       " 'dhaird',\n",
       " '3yoda',\n",
       " 'tromboman',\n",
       " 'ChrisBessert',\n",
       " 'amm',\n",
       " 'nianticgarbage',\n",
       " 'abellao',\n",
       " 'rzsi',\n",
       " 'Joseph E',\n",
       " 'Cybis',\n",
       " 'zephyr',\n",
       " 'Zenon',\n",
       " 'gormur',\n",
       " 'oanac2_telenav',\n",
       " 'TJ S',\n",
       " 'CornCO',\n",
       " 'dburnsii',\n",
       " 'StaffMapEditor',\n",
       " 'torapa',\n",
       " 'SteveC',\n",
       " 'Tyeforce',\n",
       " 'nvk',\n",
       " 'compdude',\n",
       " 'ssahithi',\n",
       " 'mitesl',\n",
       " 'bogdan_andrei',\n",
       " 'ashbeesa',\n",
       " 'endru',\n",
       " 'bsoto',\n",
       " 'alansh42',\n",
       " 'yerrawa',\n",
       " 'salix01',\n",
       " 'dfellow',\n",
       " 'moosejaw',\n",
       " 'qureahme',\n",
       " 'achantav',\n",
       " 'horndude77',\n",
       " 'jfuredy',\n",
       " 'WildwoodBob',\n",
       " 'irasesu1',\n",
       " 'andrewpmk',\n",
       " 'mJohnson89',\n",
       " 'ianmcorvidae',\n",
       " 'BugBuster',\n",
       " 'Buster452',\n",
       " 'CartoCrazy',\n",
       " 'saksudan',\n",
       " 'mw67',\n",
       " 'us-az-mesa-BVUG',\n",
       " ...}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Users\n",
    "process_users(OSM_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to CSV\n",
    "import csv\n",
    "import codecs\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "from unittest import TestCase\n",
    "import cerberus\n",
    "import schema\n",
    "\n",
    "\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the\n",
    "# sql table schema\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "# Shape each element into several data structures\n",
    "# Clean and shape node or way XML element to Python dict\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \n",
    "    node_attribs = {} \n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []\n",
    "\n",
    "    if element.tag == 'node':\n",
    "        for i in NODE_FIELDS:\n",
    "            node_attribs[i] = element.attrib[i]\n",
    "        for tag in element.iter(\"tag\"):  \n",
    "            problem = PROBLEMCHARS.search(tag.attrib['k'])\n",
    "            if not problem:\n",
    "                node_tag = {} \n",
    "                node_tag['id'] = element.attrib['id'] \n",
    "                node_tag['value'] = tag.attrib['v']  \n",
    "\n",
    "                match = LOWER_COLON.search(tag.attrib['k'])\n",
    "                if not match:\n",
    "                    node_tag['type'] = 'regular'\n",
    "                    node_tag['key'] = tag.attrib['k']\n",
    "                else:\n",
    "                    bef_colon = re.findall('^(.+):', tag.attrib['k'])\n",
    "                    aft_colon = re.findall('^[a-z|_]+:(.+)', tag.attrib['k'])\n",
    "                    node_tag['type'] = bef_colon[0]\n",
    "                    node_tag['key'] = aft_colon[0]\n",
    "                    if node_tag['type'] == \"addr\" and node_tag['key'] == \"street\":\n",
    "                        # update street name\n",
    "                        node_tag['value'] = update_name(tag.attrib['v'], mapping) \n",
    "                    elif node_tag['type'] == \"addr\" and node_tag['key'] == \"postcode\":\n",
    "                        # update post code\n",
    "                        node_tag['value'] = update_postcode(tag.attrib['v']) \n",
    "            tags.append(node_tag)\n",
    "        \n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    \n",
    "    elif element.tag == 'way':\n",
    "        for i in WAY_FIELDS:\n",
    "            way_attribs[i] = element.attrib[i]\n",
    "        for tag in element.iter(\"tag\"):\n",
    "            problem = PROBLEMCHARS.search(tag.attrib['k'])\n",
    "            if not problem:\n",
    "                way_tag = {}\n",
    "                way_tag['id'] = element.attrib['id'] \n",
    "                way_tag['value'] = tag.attrib['v']\n",
    "                match = LOWER_COLON.search(tag.attrib['k'])\n",
    "                if not match:\n",
    "                    way_tag['type'] = 'regular'\n",
    "                    way_tag['key'] = tag.attrib['k']\n",
    "                else:\n",
    "                    bef_colon = re.findall('^(.+?):+[a-z]', tag.attrib['k'])\n",
    "                    aft_colon = re.findall('^[a-z|_]+:(.+)', tag.attrib['k'])\n",
    "\n",
    "                    way_tag['type'] = bef_colon[0]\n",
    "                    way_tag['key'] = aft_colon[0]\n",
    "                    if way_tag['type'] == \"addr\" and way_tag['key'] == \"street\":\n",
    "                        way_tag['value'] = update_name(tag.attrib['v'], mapping) \n",
    "                    elif way_tag['type'] == \"addr\" and way_tag['key'] == \"postcode\":\n",
    "                        way_tag['value'] = update_postcode(tag.attrib['v']) \n",
    "            tags.append(way_tag)\n",
    "        position = 0\n",
    "        for tag in element.iter(\"nd\"):  \n",
    "            nd = {}\n",
    "            nd['id'] = element.attrib['id'] \n",
    "            nd['node_id'] = tag.attrib['ref'] \n",
    "            nd['position'] = position  \n",
    "            position += 1\n",
    "            \n",
    "            way_nodes.append(nd)\n",
    "    \n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Helper Functions            \n",
    "\n",
    "# Yield element if it is the right type of tag\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "# Raise ValidationError if element does not match schema\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "# Extend csv.DictWriter to handle Unicode input\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# Main Function                    \n",
    "\n",
    "# Iteratively process each XML element and write to csv(s)\n",
    "def process_map(file_in, validate):\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    process_map(OSM_FILE, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_map(OSM_FILE, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DB\n",
    "import csv, sqlite3\n",
    "\n",
    "con = sqlite3.connect(\"PHX_AZ1.db\")\n",
    "con.text_factory = str\n",
    "cur = con.cursor()\n",
    "\n",
    "# create nodes table\n",
    "cur.execute(\"CREATE TABLE nodes (id, lat, lon, user, uid, version, changeset, timestamp);\")\n",
    "with open('nodes.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) \n",
    "    to_db = [(i['id'], i['lat'], i['lon'], i['user'], i['uid'], i['version'], i['changeset'], i['timestamp']) \\\n",
    "             for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO nodes (id, lat, lon, user, uid, version, changeset, timestamp) \\\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?);\", to_db)\n",
    "con.commit()\n",
    "\n",
    "#create nodes_tags table\n",
    "cur.execute(\"CREATE TABLE nodes_tags (id, key, value, type);\")\n",
    "with open('nodes_tags.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) \n",
    "    to_db = [(i['id'], i['key'], i['value'], i['type']) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO nodes_tags (id, key, value, type) VALUES (?, ?, ?, ?);\", to_db)\n",
    "con.commit()\n",
    "\n",
    "#Create ways table\n",
    "cur.execute(\"CREATE TABLE ways (id, user, uid, version, changeset, timestamp);\")\n",
    "with open('ways.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) \n",
    "    to_db = [(i['id'], i['user'], i['uid'], i['version'], i['changeset'], i['timestamp']) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO ways (id, user, uid, version, changeset, timestamp) VALUES (?, ?, ?, ?, ?, ?);\", to_db)\n",
    "con.commit()\n",
    "\n",
    "#Create ways_nodes table\n",
    "cur.execute(\"CREATE TABLE ways_nodes (id, node_id, position);\")\n",
    "with open('ways_nodes.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) \n",
    "    to_db = [(i['id'], i['node_id'], i['position']) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO ways_nodes (id, node_id, position) VALUES (?, ?, ?);\", to_db)\n",
    "con.commit()\n",
    "\n",
    "#Create ways_tags table\n",
    "cur.execute(\"CREATE TABLE ways_tags (id, key, value, type);\")\n",
    "with open('ways_tags.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) \n",
    "    to_db = [(i['id'], i['key'], i['value'], i['type']) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO ways_tags (id, key, value, type) VALUES (?, ?, ?, ?);\", to_db)\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes:  43512\n",
      "Number of ways:  5974\n",
      "Number of unique users:  1231\n",
      "Top Contributing user:  (u'Dr Kludge', 10705)\n",
      "Biggest religion:  (u'christian', 3)\n",
      "popular amenity:  (u'waste_disposal', 27)\n"
     ]
    }
   ],
   "source": [
    "# Query DB\n",
    "import csv, sqlite3\n",
    "\n",
    "def number_of_nodes():\n",
    "    result = cur.execute('SELECT COUNT(*) FROM nodes')\n",
    "    return result.fetchone()[0]\n",
    "\n",
    "def number_of_ways():\n",
    "    result = cur.execute('SELECT COUNT(*) FROM ways')\n",
    "    return result.fetchone()[0]\n",
    "\n",
    "def number_of_Unique_users():\n",
    "    result = cur.execute('SELECT COUNT(distinct(uid)) FROM (SELECT uid FROM nodes UNION ALL SELECT uid FROM ways)')\n",
    "    return result.fetchone()[0]\n",
    "\n",
    "def Top_Contributing_user():\n",
    "    for row in cur.execute('SELECT e.user, COUNT(*) as num \\\n",
    "                            FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) e \\\n",
    "                            GROUP BY e.user \\\n",
    "                            ORDER BY num DESC \\\n",
    "                            LIMIT 1'):\n",
    "        return row\n",
    "\n",
    "def Biggest_religion():\n",
    "    for row in cur.execute('SELECT nodes_tags.value, COUNT(*) as num FROM nodes_tags \\\n",
    "                            JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE value=\"place_of_worship\") i \\\n",
    "                            ON nodes_tags.id=i.id \\\n",
    "                            WHERE nodes_tags.key=\"religion\" \\\n",
    "                            GROUP BY nodes_tags.value \\\n",
    "                            ORDER BY num DESC\\\n",
    "                            LIMIT 1'):\n",
    "         return row\n",
    "\n",
    "def popular_amenity():\n",
    "    for row in cur.execute('SELECT value, COUNT(*) as num \\\n",
    "                            FROM nodes_tags \\\n",
    "                            WHERE key=\"amenity\" \\\n",
    "                            GROUP BY value \\\n",
    "                            ORDER BY num DESC \\\n",
    "                            LIMIT 1'):\n",
    "        return row\n",
    "\n",
    "                \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    con = sqlite3.connect(\"PHX_AZ1.db\") \n",
    "    cur = con.cursor()\n",
    "    print \"Number of nodes: \" , number_of_nodes()\n",
    "    print \"Number of ways: \" , number_of_ways()\n",
    "    print \"Number of unique users: \" , number_of_Unique_users()\n",
    "    print \"Top Contributing user: \" , Top_Contributing_user()\n",
    "    print \"Biggest religion: \" , Biggest_religion()\n",
    "    print \"popular amenity: \" , popular_amenity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
