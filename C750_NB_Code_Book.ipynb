{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the modules that will be needed\n",
    "from collections import defaultdict\n",
    "import csv \n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "import schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Sample File\n",
    "\n",
    "\n",
    "OSM_FILE = \"map_AJ.osm\"  # Replace this with your osm file\n",
    "SAMPLE_FILE = \"sample.osm\"\n",
    "\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "k = 5 # Parameter: take every k-th top level element\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    output.write('<osm>\\n  ')\n",
    "\n",
    "    # Write every kth top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % k == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "    output.write('</osm>')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 1,\n",
      " 'member': 13747,\n",
      " 'meta': 1,\n",
      " 'nd': 281901,\n",
      " 'node': 228786,\n",
      " 'note': 1,\n",
      " 'osm': 1,\n",
      " 'relation': 157,\n",
      " 'tag': 153689,\n",
      " 'way': 37136}\n"
     ]
    }
   ],
   "source": [
    "# Iterative Parsing\n",
    "\n",
    "\n",
    "# Count Tags\n",
    "def count_tags(filename):\n",
    "    tags = {}\n",
    "    for _, elem in ET.iterparse(filename):\n",
    "        tag = elem.tag\n",
    "        if tag not in tags.keys():\n",
    "            tags[tag] = 1\n",
    "        else:\n",
    "            tags[tag] += 1\n",
    "    return tags\n",
    "\n",
    "\n",
    "# Test count tags function\n",
    "def test():\n",
    "    tags = count_tags(OSM_FILE)\n",
    "    pprint.pprint(tags)\n",
    "\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audit file\n",
    "\n",
    "# Audit street names------------------------------------------------------------------------------\n",
    "# Regular expression to check for characters at end of string, including optional period.\n",
    "# Eg \"Street\" or \"St.\"\n",
    "\n",
    "street_type_re = re.compile(r'\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "# Common street names\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Road\", \"Parkway\", \"Commons\", \"Close\", \"Highway\", \"Circle\", \"Trail\", \"US\"]\n",
    "\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.tag == \"tag\") and (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "# Iterate over the osmfile and create a dictionary mapping from expected street names\n",
    "# to collected streets.\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "\n",
    "    osm_file.close()\n",
    "    return street_types   \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {'60': {'E US Hwy 60', 'East US Highway 60'},\n",
       "             'Ave': {'E Osage Ave'},\n",
       "             'Cheshire': {'South Cheshire'},\n",
       "             'Lansing': {'S Lansing'},\n",
       "             'Rd.': {'5810 Alameda Rd.'},\n",
       "             'Saguaro': {'South Camino Saguaro'},\n",
       "             'St': {'North 99th St'}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test audit function\n",
    "audit(OSM_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping for names to be updated\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"Rd\": \"Road\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"S L\": \"South L\",\n",
    "            \"E \": \"East \",\n",
    "            \"5810 A\":\"A\",\n",
    "            \"Hwy \":\"Highway \"\n",
    "            }\n",
    "\n",
    "# Improving Street names\n",
    "def update_name(name, mapping):\n",
    "    for key in mapping.iterkeys():\n",
    "        if re.search(key, name):\n",
    "            name = re.sub(key, mapping[key], name)\n",
    "\n",
    "    return name\n",
    "\n",
    "def improve_street_name():\n",
    "    st_types = audit(OSM_FILE)\n",
    "    pprint.pprint(dict(st_types))\n",
    "\n",
    "    for st_type, ways in st_types.iteritems():\n",
    "        for name in ways:\n",
    "            better_name = update_name(name, mapping)            \n",
    "            print name, \"=>\", better_name\n",
    "    \n",
    "    #Second Pass        \n",
    "    if \"Road.\" in better_name:\n",
    "        better_name = better_name.replace(\"Road.\", \"Road\")\n",
    "        print name, \"=>\", better_name    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'60': set(['E US Hwy 60', 'East US Highway 60']),\n",
      " 'Ave': set(['E Osage Ave']),\n",
      " 'Cheshire': set(['South Cheshire']),\n",
      " 'Lansing': set(['S Lansing']),\n",
      " 'Rd.': set(['5810 Alameda Rd.']),\n",
      " 'Saguaro': set(['South Camino Saguaro']),\n",
      " 'St': set(['North 99th St'])}\n",
      "S Lansing => South Lansing\n",
      "East US Highway 60 => East US Highway 60\n",
      "E US Hwy 60 => East US Highway 60\n",
      "North 99th St => North 99th Street\n",
      "South Camino Saguaro => South Camino Saguaro\n",
      "South Cheshire => South Cheshire\n",
      "E Osage Ave => East Osage Avenue\n",
      "5810 Alameda Rd. => Alameda Road.\n",
      "5810 Alameda Rd. => Alameda Road\n"
     ]
    }
   ],
   "source": [
    "# Clean streets \n",
    "improve_street_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Postalcodes for addresses \n",
    "\n",
    "# Regular expression to check whether postalcode is in appropriate format\n",
    "postcode_re = re.compile('^[A-Z]{1,2}[0-9]{1,2}[A-Z]? [0-9][A-Z]{2}$') \n",
    "\n",
    "def is_postcode(elem):\n",
    "    return (elem.tag == \"tag\") and (elem.attrib['k'] == \"addr:postcode\")\n",
    "\n",
    "\n",
    "# Search for postcodes within \"way\" and \"node\"\n",
    "def find_postcode():\n",
    "    osm_file = open(OSM_FILE, \"r\")\n",
    "    postcode_types = set()\n",
    "    odd_postcode = set()\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_postcode(tag):\n",
    "                    m = postcode_re.search(tag.attrib['v'])\n",
    "                    if m:\n",
    "                        postcode_types.add(tag.attrib['v'])  \n",
    "                    else:\n",
    "                        odd_postcode.add(tag.attrib['v'])\n",
    "                        \n",
    "\n",
    "    osm_file.close()\n",
    "\n",
    "\n",
    "    return (postcode_types, odd_postcode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(set(),\n",
       " {'85118',\n",
       "  '85119',\n",
       "  '85120',\n",
       "  '85207',\n",
       "  '85208',\n",
       "  '85208-2305',\n",
       "  '85209',\n",
       "  '85212',\n",
       "  '85219',\n",
       "  '85220',\n",
       "  '85270'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Audit postal codes\n",
    "find_postcode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noted a postalcode with a +4 number, will drop this and maintain only 9-digit postal code\n",
    "\n",
    "area_postcode_re = re.compile('^[A-Z]{1,2}[0-9]{1,2}[A-Z]? ?[0-9]?$')\n",
    "\n",
    "def update_postcode(odd_postcode):\n",
    "    if area_postcode_re.search(odd_postcode):\n",
    "        postcode = \" \"\n",
    "    else:\n",
    "        postcode = odd_postcode.split(\"-\")[0]\n",
    "    return postcode\n",
    "\n",
    "\n",
    "def improve_postcode():\n",
    "    postcode_all = find_postcode()\n",
    "\n",
    "    for postcode in postcode_all[1]:\n",
    "        better_postcode = update_postcode(postcode)\n",
    "        print postcode, \"=>\", better_postcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85208 => 85208\n",
      "85209 => 85209\n",
      "85220 => 85220\n",
      "85219 => 85219\n",
      "85208-2305 => 85208\n",
      "85212 => 85212\n",
      "85207 => 85207\n",
      "85118 => 85118\n",
      "85120 => 85120\n",
      "85270 => 85270\n",
      "85119 => 85119\n"
     ]
    }
   ],
   "source": [
    "# Fix postal codes\n",
    "improve_postcode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "def get_user(element):\n",
    "    return\n",
    "\n",
    "# Generates list of users\n",
    "def process_users(filename):\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        name = element.attrib.get('user')\n",
    "        if name != None:\n",
    "            if name not in users:\n",
    "                users.add(name)\n",
    "                \n",
    "    pass\n",
    "    return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0xStephen',\n",
       " 'AJ Riley',\n",
       " 'Adam Martin',\n",
       " 'Adam Schneider',\n",
       " 'Adamant1',\n",
       " 'Alan Bragg',\n",
       " 'AndyAyre',\n",
       " 'Arcticmarine',\n",
       " 'ArminGh',\n",
       " 'Baloo Uriza',\n",
       " 'Bhojaraj',\n",
       " 'Bopcommander',\n",
       " 'Bryan_W',\n",
       " 'Caboosey',\n",
       " 'CamelCaseNick',\n",
       " 'Carnildo',\n",
       " 'CartoCrazy',\n",
       " 'Cato_d_Ae',\n",
       " 'Chargerrt28',\n",
       " 'Chris Bell in California',\n",
       " 'Chris Lawrence',\n",
       " 'Chris-Eleatha',\n",
       " 'ChrisMorris',\n",
       " 'Conan Brink',\n",
       " 'Cool_DPS',\n",
       " 'DannyAiquipa',\n",
       " 'Daungg',\n",
       " 'David Maciaszek',\n",
       " 'David Paleino',\n",
       " 'Derick Rethans',\n",
       " 'Dilys',\n",
       " 'Dr Kludge',\n",
       " 'DrHog',\n",
       " 'Duff614',\n",
       " 'Edward',\n",
       " 'ErichRitz',\n",
       " 'Fluffy89502',\n",
       " 'FvGordon',\n",
       " 'GREGMAP1',\n",
       " 'Gerard Jeronowitz',\n",
       " 'GerdP',\n",
       " 'Glassman',\n",
       " 'GoWestTravel',\n",
       " 'Grant Anderson',\n",
       " 'GreggTownsend',\n",
       " 'Guylamar2006',\n",
       " 'HJUdall',\n",
       " 'HoloDuke',\n",
       " 'Hoodzow',\n",
       " 'Iowa Kid',\n",
       " 'Iqhra',\n",
       " 'Jamie Mueller',\n",
       " 'Jesse Hamlin',\n",
       " 'JesseFW',\n",
       " 'Jon Hanson',\n",
       " 'JulienBalas',\n",
       " 'KR-KRKR-KR',\n",
       " 'KinkyKinkles',\n",
       " 'KripaluShanti',\n",
       " 'KristenK',\n",
       " 'LastNameConnors',\n",
       " 'Luis36995',\n",
       " 'Map King',\n",
       " 'MapClick',\n",
       " 'MatthewAndersonUS80',\n",
       " 'Megan A',\n",
       " 'MikeChuck',\n",
       " 'Minh Nguyen',\n",
       " 'MisterPhilip',\n",
       " 'NE2',\n",
       " 'NE3',\n",
       " 'NeverGuy',\n",
       " 'Osmosefixer2021',\n",
       " 'PHerison',\n",
       " 'ParagonPrime',\n",
       " 'PlacesOfInterest2887',\n",
       " 'Polarbear',\n",
       " 'PrivatePawn',\n",
       " 'Rangaofdeath',\n",
       " 'Reino Baptista',\n",
       " 'RetiredInNH',\n",
       " 'RichRico',\n",
       " 'Richard',\n",
       " 'RoadGeek_MD99',\n",
       " 'RockerDadAZ',\n",
       " 'Rudolf Mayer',\n",
       " 'Sal_W',\n",
       " 'SimMoonXP',\n",
       " 'Singvogel',\n",
       " 'SomeoneElse',\n",
       " 'SomeoneElse_Revert',\n",
       " 'StellanL',\n",
       " 'Stephen214',\n",
       " 'Sundance',\n",
       " 'SuperZambezi',\n",
       " 'Taroc',\n",
       " 'Teesta',\n",
       " 'TeresaPeteti',\n",
       " 'TheDutchMan13',\n",
       " 'Thyais Meade',\n",
       " 'Timothy Smith',\n",
       " 'TorCguy',\n",
       " 'TravGW',\n",
       " 'Vkkalegi',\n",
       " 'Zruda',\n",
       " 'abel801',\n",
       " 'abellao',\n",
       " 'abhap',\n",
       " 'abvincen',\n",
       " 'accheela',\n",
       " 'achantav',\n",
       " 'addatla',\n",
       " 'adenium',\n",
       " 'adhya1',\n",
       " 'aggopak',\n",
       " 'agnisn',\n",
       " 'ahabdu',\n",
       " 'akalohri',\n",
       " 'almaasm',\n",
       " 'amarajz',\n",
       " 'ambarapu',\n",
       " 'amillar',\n",
       " 'amitbish',\n",
       " 'amithrav',\n",
       " 'ammendir',\n",
       " 'ammhita',\n",
       " 'amzmkuma',\n",
       " 'amznruth',\n",
       " 'andreis_telenav',\n",
       " 'andrewpmk',\n",
       " 'anilredd',\n",
       " 'animebirder',\n",
       " 'antoalvi',\n",
       " 'anukrits',\n",
       " 'appank',\n",
       " 'arehaan',\n",
       " 'arkdatta',\n",
       " 'arosalon',\n",
       " 'arpremna',\n",
       " 'asddiqu',\n",
       " 'ashbeesa',\n",
       " 'ashleyannmathew',\n",
       " 'ashradha',\n",
       " 'asrdd',\n",
       " 'attaragi',\n",
       " 'aurel_joys',\n",
       " 'avssr',\n",
       " 'b-jazz',\n",
       " 'b-jazz-bot',\n",
       " 'bachurev',\n",
       " 'baditaflorin',\n",
       " 'bahnpirat',\n",
       " 'baigzake',\n",
       " 'baindran',\n",
       " 'bal_agates',\n",
       " 'balrog-kun',\n",
       " 'bapup',\n",
       " 'bathis',\n",
       " 'befit1',\n",
       " 'begumfai',\n",
       " 'bhardwk',\n",
       " 'bhavana naga',\n",
       " 'bhavant',\n",
       " 'bhiy',\n",
       " 'biancah_telenav',\n",
       " 'billy steele',\n",
       " 'bjnlm',\n",
       " 'bkil',\n",
       " 'blong77435',\n",
       " 'bmmh',\n",
       " 'bogdan_andrei',\n",
       " 'borvikas',\n",
       " 'brycedb74',\n",
       " 'bs_42',\n",
       " 'buchula',\n",
       " 'buddhirv',\n",
       " 'byronigoe',\n",
       " 'calfarome',\n",
       " 'cassini83',\n",
       " 'cdavila',\n",
       " 'chachafish',\n",
       " 'chagasru',\n",
       " 'chandeek',\n",
       " 'changurl',\n",
       " 'chatrags',\n",
       " 'chaturvh',\n",
       " 'chepteja',\n",
       " 'chosayan',\n",
       " 'compdude',\n",
       " 'corb555',\n",
       " 'corinag_telenav',\n",
       " 'craigloftus',\n",
       " 'cswaroo',\n",
       " 'daggarw',\n",
       " 'datbrahm',\n",
       " 'dchiles',\n",
       " 'dcp',\n",
       " 'deanabil',\n",
       " 'debanka',\n",
       " 'debott',\n",
       " 'deepikja',\n",
       " 'desuman',\n",
       " 'devapujk',\n",
       " 'devratc',\n",
       " 'dgitto',\n",
       " 'dhaird',\n",
       " 'dhakanth',\n",
       " 'discountshedsllc',\n",
       " 'dobaer',\n",
       " 'doublah',\n",
       " 'dreadfyre',\n",
       " 'duarisha',\n",
       " 'dviraja',\n",
       " 'dvyasha',\n",
       " 'dyellak',\n",
       " 'eadamk',\n",
       " 'ecloud',\n",
       " 'ediyes',\n",
       " 'ejjoshy',\n",
       " 'ekkakkas',\n",
       " 'elbatrop',\n",
       " 'eric22',\n",
       " 'fredjunod',\n",
       " 'freebeer',\n",
       " 'ganarend',\n",
       " 'gappleto97',\n",
       " 'gauraan',\n",
       " 'gazsri',\n",
       " 'gdodlapa',\n",
       " 'gilasri',\n",
       " 'gireeshn',\n",
       " 'gohimans',\n",
       " 'gollabg',\n",
       " 'goraia',\n",
       " 'govvala',\n",
       " 'gpserror',\n",
       " 'grandhs',\n",
       " 'gsravya1',\n",
       " 'guidorice',\n",
       " 'gujelapg',\n",
       " 'gundanar',\n",
       " 'harishwr',\n",
       " 'harpreo',\n",
       " 'harshjs',\n",
       " 'herbm123',\n",
       " 'himansne',\n",
       " 'hofoen',\n",
       " 'hoream_telenav',\n",
       " 'ialex',\n",
       " 'iandees',\n",
       " 'iggujja',\n",
       " 'inagans',\n",
       " 'inah_telenav',\n",
       " 'indukun',\n",
       " 'istvanv_telenav',\n",
       " 'jacorohi',\n",
       " 'jaiaka',\n",
       " 'jamenimm',\n",
       " 'javmoh',\n",
       " 'jaywhyenecks',\n",
       " 'jdoniche',\n",
       " 'jds34',\n",
       " 'jedidy',\n",
       " 'jfuredy',\n",
       " 'jhameena',\n",
       " 'jimbo333',\n",
       " 'jinalfoflia',\n",
       " 'jogger333',\n",
       " 'jojoyal',\n",
       " 'jomsjaco',\n",
       " 'jonathanpatt',\n",
       " 'jonesydesign',\n",
       " 'jordanhoon',\n",
       " 'jpristelski',\n",
       " 'jyotkuma',\n",
       " 'kammann',\n",
       " 'kanleela',\n",
       " 'kararcha',\n",
       " 'karitotp',\n",
       " 'karl-marx',\n",
       " 'karnatim',\n",
       " 'karragha',\n",
       " 'kaurrupa',\n",
       " 'kavshnu',\n",
       " 'kdkgv',\n",
       " 'kedarnap',\n",
       " 'kevmath',\n",
       " 'kghazi',\n",
       " 'kirashas',\n",
       " 'kirrawat',\n",
       " 'kkoganti',\n",
       " 'kkre1',\n",
       " 'kllahari',\n",
       " 'knikitha',\n",
       " 'kollipay',\n",
       " 'kookiemonster',\n",
       " 'kotaprad',\n",
       " 'kshtdh',\n",
       " 'ksnghr',\n",
       " 'kumartcx',\n",
       " 'kvss',\n",
       " 'laharicl',\n",
       " 'leelamrs',\n",
       " 'limmmoh',\n",
       " 'lorandr_telenav',\n",
       " 'loyakris',\n",
       " 'lukas64',\n",
       " 'mahmedqg',\n",
       " 'majumdea',\n",
       " 'malenki',\n",
       " 'manitec',\n",
       " 'mansipan',\n",
       " 'manuelab_telenav',\n",
       " 'mapalla',\n",
       " 'maponpoint33',\n",
       " 'marianp_telenav',\n",
       " 'markzawi',\n",
       " 'marthaleena',\n",
       " 'mathpras',\n",
       " 'matthieun',\n",
       " 'maxerickson',\n",
       " 'mcingara',\n",
       " 'mdalinis',\n",
       " 'melisramer',\n",
       " 'mesatowingcompany',\n",
       " 'metaniq',\n",
       " 'michaedz',\n",
       " 'mihaii_telenav',\n",
       " 'mike140',\n",
       " 'mitesl',\n",
       " 'mneko',\n",
       " 'modukv',\n",
       " 'mohahase',\n",
       " 'mohapd',\n",
       " 'movemus',\n",
       " 'mrfa',\n",
       " 'mueschel',\n",
       " 'musaibm',\n",
       " 'muziriana',\n",
       " 'mvviveka',\n",
       " 'mwakram',\n",
       " 'mzamam',\n",
       " 'n76',\n",
       " 'naaitha',\n",
       " 'nairnidh',\n",
       " 'nambim',\n",
       " 'nandinab',\n",
       " 'naresksv',\n",
       " 'navlay',\n",
       " 'navuddin',\n",
       " 'nbhiss',\n",
       " 'nedluxaca',\n",
       " 'nehnaaz',\n",
       " 'nicolas17',\n",
       " 'nimmalab',\n",
       " 'nishisi',\n",
       " 'nmixter',\n",
       " 'nnpriyn',\n",
       " 'nstefan_telenav',\n",
       " 'ntanniru',\n",
       " 'nyuriks',\n",
       " 'oanac2_telenav',\n",
       " 'oldtopos',\n",
       " 'oliver01',\n",
       " 'ommundu',\n",
       " 'ortho_is_hot',\n",
       " 'paasthan',\n",
       " 'panchis1',\n",
       " 'patadivy',\n",
       " 'pathkh',\n",
       " 'patrick228',\n",
       " 'pattamatta',\n",
       " 'payelgh',\n",
       " 'pbellamk',\n",
       " 'pbobbili',\n",
       " 'pbudhara',\n",
       " 'pdantojh',\n",
       " 'pddondet',\n",
       " 'penimire',\n",
       " 'petrar_telenav',\n",
       " 'pezizomycotina',\n",
       " 'pflier',\n",
       " 'phxcpugeek254',\n",
       " 'phxcpugeek354',\n",
       " 'piligab',\n",
       " 'pillalp',\n",
       " 'ppallam',\n",
       " 'ppjj',\n",
       " 'pppadal',\n",
       " 'praghath',\n",
       " 'pramvyas',\n",
       " 'pratikyadav',\n",
       " 'presakha',\n",
       " 'qureahme',\n",
       " 'racranji',\n",
       " 'rahuzod',\n",
       " 'raknm',\n",
       " 'ramijcr',\n",
       " 'ranjithjoy',\n",
       " 'rapush',\n",
       " 'rathjoth',\n",
       " 'raushiferum',\n",
       " 'ravsjith',\n",
       " 'rawatg',\n",
       " 'redgnana',\n",
       " 'rekasruj',\n",
       " 'rekulc1',\n",
       " 'reswara',\n",
       " 'revent',\n",
       " 'rickmastfan67',\n",
       " 'ridixcr',\n",
       " 'rithikaj',\n",
       " 'riysingh',\n",
       " 'rkkarnee',\n",
       " 'rkkasams',\n",
       " 'rojganes',\n",
       " 'rppalagu',\n",
       " 'russdeffner',\n",
       " 'ruthmaben',\n",
       " 'rzsi',\n",
       " 'sabaamzn',\n",
       " 'sabok',\n",
       " 'sachinv',\n",
       " 'sainiyam',\n",
       " 'saksudan',\n",
       " 'samely',\n",
       " 'sandchi',\n",
       " 'sanganh',\n",
       " 'sankethn',\n",
       " 'sartsuri',\n",
       " 'sawhs',\n",
       " 'saxvidit',\n",
       " 'scharith',\n",
       " 'schlammbad',\n",
       " 'seanking2919',\n",
       " 'sebastic',\n",
       " 'sggundl',\n",
       " 'sgmamz',\n",
       " 'sgudiva',\n",
       " 'shabkhat',\n",
       " 'shadijan',\n",
       " 'shadty',\n",
       " 'shark_dentist96',\n",
       " 'shkshar',\n",
       " 'shoshibu',\n",
       " 'shushmit',\n",
       " 'siayu',\n",
       " 'siddhaam',\n",
       " 'silakshm',\n",
       " 'sinreeya',\n",
       " 'sireeshp',\n",
       " 'sivachak',\n",
       " 'sivaib',\n",
       " 'sivapaid',\n",
       " 'skodeboi',\n",
       " 'skquinn',\n",
       " 'smbharan',\n",
       " 'snghcvv',\n",
       " 'snyash',\n",
       " 'soumybha',\n",
       " 'soundred',\n",
       " 'sreejam',\n",
       " 'sreelekk',\n",
       " 'sriharsd',\n",
       " 'srirohan',\n",
       " 'srividya_c',\n",
       " 'srmudava',\n",
       " 'srygy',\n",
       " 'ssahithi',\n",
       " 'ssange',\n",
       " 'sskalyan',\n",
       " 'sunaman',\n",
       " 'sunilaak',\n",
       " 'sushredd',\n",
       " 'svvakkal',\n",
       " 'swappa',\n",
       " 'swarnimv',\n",
       " 'swathiku',\n",
       " 'swimdb',\n",
       " 'syrah1',\n",
       " 'tahuram',\n",
       " 'talukdm',\n",
       " 'tammpava',\n",
       " 'team oggy',\n",
       " 'tejajt',\n",
       " 'tejkante',\n",
       " 'teliks',\n",
       " 'tmadhuli',\n",
       " 'tomthepom',\n",
       " 'troyras673',\n",
       " 'tusnayak',\n",
       " 'unnsyeda',\n",
       " 'us-az-mesa-UGBL',\n",
       " 'user_5359',\n",
       " 'uthkarsh1',\n",
       " 'utkap',\n",
       " 'utkshukl',\n",
       " 'vaidhev',\n",
       " 'vardhamk',\n",
       " 'varmasa',\n",
       " 'vasamd',\n",
       " 'vatsomya',\n",
       " 'vbbukka',\n",
       " 'vchndl',\n",
       " 'veesams',\n",
       " 'velurib',\n",
       " 'venkakaz',\n",
       " 'venkanna37',\n",
       " 'vinitht',\n",
       " 'vissushm',\n",
       " 'vkanduku',\n",
       " 'vkatredd',\n",
       " 'vkkasarl',\n",
       " 'vpprahar',\n",
       " 'vsahith',\n",
       " 'vvrddy',\n",
       " 'vvvnm',\n",
       " 'vvvuppal',\n",
       " 'vysyarp',\n",
       " 'wmann',\n",
       " 'wolfgang8741',\n",
       " 'woodpeck',\n",
       " 'woodpeck_fixbot',\n",
       " 'woodpeck_repair',\n",
       " 'yadathot',\n",
       " 'yerrawa',\n",
       " 'ygudeti',\n",
       " 'yurasi',\n",
       " 'zephyr',\n",
       " 'zmankits',\n",
       " u'\\u042e\\u043a\\u0430\\u0442\\u0430\\u043d'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Users\n",
    "process_users(OSM_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Tags \n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "# Regex\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "# Checks tags for different kinds of characters and formats\n",
    "def key_type(element, keys):\n",
    "    if element.tag == 'tag':\n",
    "        if lower.search(element.attrib['k']):\n",
    "            keys['lower'] += 1\n",
    "        elif lower_colon.search(element.attrib['k']):\n",
    "            keys['lower_colon'] += 1\n",
    "        elif problemchars.search(element.attrib['k']):\n",
    "            keys['problemchars'] += 1\n",
    "        else:\n",
    "            keys['other'] += 1\n",
    "        pass\n",
    "    \n",
    "    return keys\n",
    "\n",
    "# Main function - counts up number of different types of tags\n",
    "def process_keys(filename):\n",
    "    keys = {'lower': 0, 'lower_colon': 0, 'problemchars': 0, 'other': 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "        \n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lower': 92690, 'lower_colon': 57758, 'other': 3241, 'problemchars': 0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_keys(OSM_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to CSV\n",
    "import csv\n",
    "import codecs\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "from unittest import TestCase\n",
    "import cerberus\n",
    "import schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the\n",
    "# sql table schema\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "# looks for the incorrect street types in the street names by comparing them to the \"expected\" list\n",
    "# and then puts them in a list called street_types\n",
    "# uses the regular expression \"street_type_re\" defined prevously to locate the street type within the street name\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "            \n",
    "# finds the street names in the map.xml file\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "# executes the audit_street_type and is_street_name functions to fill the street_types dictionary\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "# fixes the street type in the street name\n",
    "def update_name(name, mapping):\n",
    "    m = street_type_re.search(name)\n",
    "    if m:\n",
    "        for i in mapping:\n",
    "            if i == m.group():\n",
    "                name = re.sub(street_type_re, mapping[i], name)\n",
    "    return name\n",
    "\n",
    "# finds the zip codes in the address \n",
    "def is_postcode(elem): \n",
    "    return (elem.attrib['k'] == \"addr:postcode\" or elem.attrib['k'] == \"postal_code\")\n",
    "\n",
    "# creates a list of zipcodes\n",
    "def audit_postcode(postcodes, postcode):\n",
    "    postcodes[postcode].add(postcode)\n",
    "    return postcodes\n",
    "\n",
    "# updates/cleans the zipcodes \n",
    "def update_postcode(postcode):\n",
    "    if re.findall(r'^\\d{5}$', postcode): # 5 digits\n",
    "        valid_postcode = postcode\n",
    "        return valid_postcode  \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Shape each element into several data structures\n",
    "# Clean and shape node or way XML element to Python dict\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \n",
    "    node_attribs = {} \n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []\n",
    "\n",
    "    if element.tag == 'node':\n",
    "        for i in NODE_FIELDS:\n",
    "            node_attribs[i] = element.attrib[i]\n",
    "        for tag in element.iter(\"tag\"):  \n",
    "            problem = PROBLEMCHARS.search(tag.attrib['k'])\n",
    "            if not problem:\n",
    "                node_tag = {} \n",
    "                node_tag['id'] = element.attrib['id'] \n",
    "                node_tag['value'] = tag.attrib['v']  \n",
    "\n",
    "                match = LOWER_COLON.search(tag.attrib['k'])\n",
    "                if not match:\n",
    "                    node_tag['type'] = 'regular'\n",
    "                    node_tag['key'] = tag.attrib['k']\n",
    "                else:\n",
    "                    bef_colon = re.findall('^(.+):', tag.attrib['k'])\n",
    "                    aft_colon = re.findall('^[a-z|_]+:(.+)', tag.attrib['k'])\n",
    "                    node_tag['type'] = bef_colon[0]\n",
    "                    node_tag['key'] = aft_colon[0]\n",
    "                    if node_tag['type'] == \"addr\" and node_tag['key'] == \"street\":\n",
    "                        # update street name\n",
    "                        node_tag['value'] = update_name(tag.attrib['v'], mapping) \n",
    "                    elif node_tag['type'] == \"addr\" and node_tag['key'] == \"postcode\":\n",
    "                        # update post code\n",
    "                        node_tag['value'] = update_postcode(tag.attrib['v']) \n",
    "            tags.append(node_tag)\n",
    "        \n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    \n",
    "    elif element.tag == 'way':\n",
    "        for i in WAY_FIELDS:\n",
    "            way_attribs[i] = element.attrib[i]\n",
    "        for tag in element.iter(\"tag\"):\n",
    "            problem = PROBLEMCHARS.search(tag.attrib['k'])\n",
    "            if not problem:\n",
    "                way_tag = {}\n",
    "                way_tag['id'] = element.attrib['id'] \n",
    "                way_tag['value'] = tag.attrib['v']\n",
    "                match = LOWER_COLON.search(tag.attrib['k'])\n",
    "                if not match:\n",
    "                    way_tag['type'] = 'regular'\n",
    "                    way_tag['key'] = tag.attrib['k']\n",
    "                else:\n",
    "                    bef_colon = re.findall('^(.+?):+[a-z]', tag.attrib['k'])\n",
    "                    aft_colon = re.findall('^[a-z|_]+:(.+)', tag.attrib['k'])\n",
    "\n",
    "                    way_tag['type'] = bef_colon[0]\n",
    "                    way_tag['key'] = aft_colon[0]\n",
    "                    if way_tag['type'] == \"addr\" and way_tag['key'] == \"street\":\n",
    "                        way_tag['value'] = update_name(tag.attrib['v'], mapping) \n",
    "                    elif way_tag['type'] == \"addr\" and way_tag['key'] == \"postcode\":\n",
    "                        way_tag['value'] = update_postcode(tag.attrib['v']) \n",
    "            tags.append(way_tag)\n",
    "        position = 0\n",
    "        for tag in element.iter(\"nd\"):  \n",
    "            nd = {}\n",
    "            nd['id'] = element.attrib['id'] \n",
    "            nd['node_id'] = tag.attrib['ref'] \n",
    "            nd['position'] = position  \n",
    "            position += 1\n",
    "            \n",
    "            way_nodes.append(nd)\n",
    "    \n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Helper Functions            \n",
    "\n",
    "# Yield element if it is the right type of tag\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "# Raise ValidationError if element does not match schema\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "# Extend csv.DictWriter to handle Unicode input\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# Main Function                    \n",
    "\n",
    "# Iteratively process each XML element and write to csv(s)\n",
    "def process_map(file_in, validate):\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    process_map(OSM_FILE, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_map(OSM_FILE, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
